---
params:
  projectdir: !expr paste0("/mnt/", Sys.getenv("STUDY_ID_DIR"))
  project_name: !expr Sys.getenv("STUDY_ID_DIR")
  countmatdir: !expr paste0("/mnt/", Sys.getenv("STUDY_ID_DIR"), "/output/", Sys.getenv("ALIGN_QUANT_METHOD"))
  clust_method: "spearman" # For clustering
  tree_height_cutoff: 0.1 # For clustering
  dendro_color_by: "Dose" # Specify how you would like to color the dendrograms
  nmr_threshold: 25 # 10% of 1M reads for TempOSeq = 100,000; 10% of 10M reads for RNA-Seq = 1,000,000.
  align_threshold: 0.7 # 50% alignment rate for Targeted-RNAseq Experiments, 70% for RNA-seq
  gini_cutoff: 0.99 # If dataset has no replicates, set gini to 1... DEFAULT TO (0.99)
  q30_cutoff: 0.7 # Discard samples if % of Q scores ≥ 30 was less than 70% by default
  forward_reverse_q30_diff_cutoff: 0.25 # Discard samples if the difference in the q30_percentage is greater than 0.25 by default. This is not necessary for single reads.
  PCA_cutoff: 0.2 # Default is 20%. Sample not clustering with their dose replicates (>20% variance) are removed
  sampledata_sep: "\t" # Comma for TempO-Seq, Maybe tabs for RNASeq, customize!
  groups: ["Compound", "Dose"] # These should be "interesting" groups for your analysis. Group together for exploring covariation.
  batch_var: NULL # "batch"
  dose: "Dose" # If there is a dose in the experiment, e.g., "Dose"; otherwise use NULL
  treatment_var: "Compound"
  Platform: "RNA-Seq" # TempO-Seq Or RNA-Seq
  technical_control: "technical_control" # Column names for metadata, if applicable
  reference_rna: "reference_rna" # Column names for metadata, if applicable
  solvent_control: "solvent_control" # Column names for metadata, if applicable
output:
  html_document:
    toc: true
    toc_float: true
    number_sections: true
    code_folding: hide
    theme: spacelab # flatly spacelab sandstone cerulean
    code_download: true
bibliography: "`r file.path('/mnt', Sys.getenv('STUDY_ID_DIR'), 'Rmd', 'references.bib')`"
title: "`r paste('Study-wide sample quality control: ', gsub(pattern = '_', replacement = ' ', x = Sys.getenv('STUDY_ID_DIR')), ' - ', Sys.getenv('ALIGN_QUANT_METHOD'))`"
author: "Jory Curry"
---

# `r paste('Study-wide sample quality control:', gsub(pattern = '_', replacement = ' ', x = Sys.getenv('STUDY_ID_DIR')), ' - ', Sys.getenv('ALIGN_QUANT_METHOD'))`

***

This report is generated for the project `r params$project_name` located at `r params$projectdir`.

Modified from code provided by Andrew Williams (Health Canada) and recommendations from Joshua Harrill (US EPA). See [References].  

This should be run prior to DEG/BMD analysis to remove any suspect samples from across the entire study.  

The metadata you use should have the following columns with T or F listed for each sample:  

* `technical_control`  
* `reference_rna`  
* `solvent_control`  

***

```{r params_define_in_testing, message=F, warnings=F, echo=F, eval=F}
# Create the params list object
params <- list(
  projectdir = paste0("/mnt/", Sys.getenv("STUDY_ID_DIR")),
  project_name = Sys.getenv("STUDY_ID_DIR"),
  countmatdir = paste0("/mnt/", Sys.getenv("STUDY_ID_DIR"), "/output/", Sys.getenv("ALIGN_QUANT_METHOD")),
  clust_method = "spearman",           # For clustering
  tree_height_cutoff = 0.8,            # For clustering
  dendro_color_by = "Dose",            # Specify how you would like to color the dendrograms
  nmr_threshold = 25,                  # 10% of 1M reads for TempOSeq = 100,000; 10% of 10M reads for RNA-Seq = 1,000,000
  align_threshold = 0.1,               # 50% alignment rate for Targeted-RNAseq Experiments, 70% for RNA-seq
  gini_cutoff = 0.99,                  # If dataset has no replicates, set gini to 1... DEFAULT TO (0.99)
  q30_cutoff = 0.1,                    # Discard samples if % of Q scores ≥ 30 was less than 70% by default
  forward_reverse_q30_diff_cutoff = 0.25, # Discard samples if the difference in the q30_percentage is greater than 0.25 by default
  PCA_cutoff = 0.8,                    # Default is 20%. Sample not clustering with their dose replicates (>20% variance) are removed
  sampledata_sep = "\t",               # Comma for TempO-Seq, Maybe tabs for RNASeq, customize!
  groups = list("Compound", "Dose"),   # These should be "interesting" groups for your analysis
  batch_var = NULL,                    # "batch"
  dose = "Dose",                       # If there is a dose in the experiment, e.g., "Dose"; otherwise use NULL
  treatment_var = "Compound",
  Platform = "RNA-Seq",                # TempO-Seq Or RNA-Seq
  technical_control = "technical_control", # Column names for metadata, if applicable
  reference_rna = "reference_rna",     # Column names for metadata, if applicable
  solvent_control = "solvent_control"  # Column names for metadata, if applicable
)

```

```{r load_libraries, message = F, warnings = F, echo = F}


############################################################################
# Libraries
############################################################################

#### Record start time
startTime <- Sys.time()

#Making your own conda environment with all the necessary packages...
#conda create --name my_r_pkgs --clone r_pkgs
#conda install -c conda-forge r-packagename OR
#conda install -c bioconda bioconductor-packagename
#NOTE: use the updated_my_r_pkgs conda env
#personal_lib_path <- "~/R_libs"

library(edgeR)
library(ggplot2)
library(data.table)
library(cluster)
#library(extrafont)
#library(showtextdb)
#library(showtext)
library(Cairo)
library(jsonlite)
library(sfsmisc)
library(fields)
library(heatmap3)
library(DESeq2)
library(tidyverse)
library(foreach)
library(doParallel)
library(UpSetR)
library(GGally)
library(pheatmap)
#library(ComplexHeatmap, lib.loc = "~/R_libs")
library(kableExtra)
library(dendextend)
library(dendsort)
library(rrcov)
library(cellWise)
library(vtree)
#library(here)
library(devtools)
```


```{r params, include = FALSE, echo = FALSE, message = FALSE}
projectdir <- params$projectdir
count_matrix_dir <- params$countmatdir
chemical_name <- sub("^.*Study_id_\\d+_([^_]+)_\\d+(\\.\\d+)?_day_exposure$", "\\1", projectdir)
clust_method <- params$clust_method
tree_height_cutoff <- params$tree_height_cutoff
dendro_color_by <- params$dendro_color_by
nmr_threshold <- params$nmr_threshold
align_threshold <- params$align_threshold
gini_cutoff <- params$gini_cutoff
q30_cutoff <- params$q30_cutoff
forward_reverse_q30_diff_cutoff <-
  params$forward_reverse_q30_diff_cutoff
PCA_cutoff <- params$PCA_cutoff
sampledata_sep <- params$sampledata_sep
groups <- params$groups
#groups[[2]]: c("I7_Index_ID","I5_Index_ID")
#groups[[3]]: c("Row","Column")
#groups[[4]]: c("Batch")
groups <-
  unname(groups) # Include b/c of how things are addressed throughout.
batch_var <- params$batch_var
dose <- params$dose
treatment_var <- params$treatment_var
Platform <- params$Platform
# Column names for metadata, if applicable
technical_control <- params$technical_control
reference_rna <- params$reference_rna
solvent_control <- params$solvent_control
```

```{r paths}
############################################################################
# File paths in project directory
############################################################################
projectdir <- projectdir
paths <- list()
paths$root <- "/mnt"
paths$data <- projectdir
paths$output <- paste0(paths$data, "/output")
paths$processed <-
  count_matrix_dir #This directory should be contain the count table output by R-ODAF_1_sequencing_DataPreprocess...
paths$metadata <-
  paths$root #Back to GSE Accession ID parent directory that contains the metadata file, e.g., normalizePath(file.path(paths$root, ".."))
# Need to update this path... temporary for testing purposes
paths$reports <- paste0(paths$data, "/reports")
if (!dir.exists(paths$reports)) {
  print("Creating Report dir")
  dir.create(paths$reports)
} else {
  print("Report dir exists.")
}

knitr::opts_knit$set(root.dir = paths$root)
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
options(browser = "false") # Set browser option to a dummy value because I am in an HPC environment that does not have a browser

# Function to safely include graphics
include_safe_graphics <- function(file_path) {
  absolute_path <- normalizePath(file_path, mustWork = FALSE)
  if (file.exists(absolute_path)) {
    knitr::include_graphics(absolute_path, error = FALSE)
  } else {
    message(paste("File does not exist:", absolute_path))
  }
}
```

```{r load_files, message = F, results = "hide"}
############################################################################
# FILES TO LOAD
############################################################################
# A. Tab delimited count matrix with at least 2 columns:
SampleDataFile <-
  normalizePath(file.path(paths$processed, "genes.data.tsv")) #projectdir/output/count_matrix_dir/genes.data.tsv
sampleData <- read.table(
  SampleDataFile,
  sep = sampledata_sep,
  #Typically \t
  stringsAsFactors = FALSE,
  header = TRUE,
  quote = "\"",
  row.names = 1,
  #first col is rownames
  check.names = FALSE  
  )
mu <- NULL

# B. Tab delimited sample information file with at least 2 columns:
#    1. sample names identical to the column names of sampleData
#    2. compound/group/whatever (needs to identify to which experimental group the sample belongs) #We will have compound and Dose in otu SRATables
SampleKeyFile <-
  normalizePath(file.path(paths$metadata, "SraRunTable.csv")) #Up one from the Study_id directory in root corresponding to the ACCESSION ID

DESeqDesign <- read.delim(
  SampleKeyFile,
  stringsAsFactors = FALSE,
  sep = ",",
  #SRATables are comma-separated
  header = TRUE,
  quote = "\"",
  row.names = NULL
)

# Function to check if all colnames of sampleData end with "_1" and remove the suffix
clean_sample_names <- function(sampleData, DESeqDesign) {
  # Check if all column names end with "_1"
  if (all(grepl("_1$", colnames(sampleData)))) {
    # Remove "_1" from the column names
    colnames(sampleData) <- sub("_1$", "", colnames(sampleData))
    
    # Verify that the names now match between sampleData and DESeqDesign$Run
    if (all(colnames(sampleData) %in% DESeqDesign$Run)) {
      message("Sample names successfully cleaned and matched with metadata.")
    } else {
      warning("Sample names were cleaned, but do not match the metadata.")
    }
  } else {
    message("Sample names do not require cleaning. No changes made.")
  }
  
  return(sampleData)
}

# Apply the function to clean sample names
sampleData <- clean_sample_names(sampleData, DESeqDesign)

temp_samplenames <- names(sampleData)
DESeqDesign$original_names <-
  DESeqDesign[, 1] #This assumes the Run name is in the first column
DESeqDesign <-
  DESeqDesign[DESeqDesign$original_names %in% temp_samplenames,] #Filtering the DESeqDesign to just be the samples in this dataset... useful for metadata containing data from multiple experiments/study ids
DESeqDesign$Compound <- gsub(" ", "", DESeqDesign$Compound) #Removing spaces that may have been in the metadata which can screw up this filtering process...
DESeqDesign <- 
   DESeqDesign[DESeqDesign$Compound == chemical_name, ] #Filtering the DESeqDesign by chemical name present in the project directory name


############################################################################
# Arrange tables by sample names
############################################################################

sampleData <-
  sampleData %>% dplyr::relocate(colnames(sampleData) %>% sort())
ncol(sampleData)

#design
d <- DESeqDesign
d <- d %>% dplyr::arrange(original_names)
nrow(d)

############################################################################
#Checking row and column orderings
############################################################################
all(names(sampleData) %in% d$original_names)
all(names(sampleData) == d$original_names)
d <- d[d$original_names %in% names(sampleData), ]
DESeqDesign <- d
all(names(sampleData) == d$original_names)
```

# Sample Filtering {.tabset}

## Q30 score (Phred score)

Base Q-scores of 30 or above have a base calling accuracy of 99.9%+ A Q-score of 20 corresponds to just 99% accuracy. Therefore samples with an average Q-score of 30 are of excellent quality. The R-ODAF default cutoff threshold for removing samples with poor quality is 70%. This QC report uses a q30 cutoff of `r q30_cutoff*100`%. 

For paired-end data, if the difference between the sample's Q30 percentage from the forward and reverse reads are more than 25% by default, the sample is flagged for removal. This report uses a forward-reverse Q30 difference cutoff of `r forward_reverse_q30_diff_cutoff*100`%.

```{r q30_filtering, eval=TRUE, warning=FALSE, fig.height=8, fig.width=10, out.width='100%', out.height='100%'}


#NOTE: Might need change for TempO-Seq

############################################################################
# Flagging samples based on their Quality/Phred scores!
############################################################################
#params$q30_cutoff
#params$forward_reverse_q30_diff_cutoff

parse_fastp_json <- function(json_dir, q30_params) {
  # Check if directory exists and contains JSON files
  if (!dir.exists(json_dir) ||
      length(list.files(json_dir, pattern = "\\.fastp\\.json$", full.names = TRUE)) == 0) {
    print(
      "Q30 scores are not available in JSON files to analyze in the fastpQC output directory, please review."
    )
    return(NULL)
  }
  
  # Helper function to calculate Q30 percentages
  calculate_q30_percentage <- function(data) {
    list(
      before_filtering = data$summary$before_filtering$q30_bases / data$summary$before_filtering$total_bases * 100,
      after_filtering = data$summary$after_filtering$q30_bases / data$summary$after_filtering$total_bases * 100
    )
  }
  
  # Initialize an empty data frame
  results <- data.frame(
    sample_name = character(),
    q30_before = numeric(),
    q30_after = numeric(),
    read1_q30_after = numeric(),
    read2_q30_after = numeric(),
    passes_q30_threshold = logical(),
    diff_r1_r2_before = numeric(),
    diff_r1_r2_after = numeric(),
    passes_diff_threshold = logical(),
    is_paired_end = logical(),
    stringsAsFactors = FALSE
  )
  
  # List all JSON files in the directory, excluding the log file
  json_files <- list.files(json_dir, pattern = "\\.fastp\\.json$", full.names = TRUE)
  
  message(paste("Processing", length(json_files), "JSON files..."))
  
  # Process each file, skipping any that are corrupted
  for (i in seq_along(json_files)) {
    file <- json_files[i]
    sample_name <- gsub("\\.fastp\\.json$", "", basename(file))
    
    message(paste0("Processing file ", i, " of ", length(json_files), ": ", sample_name))
    
    # Use tryCatch to handle potential errors when reading JSON files
    json_data <- tryCatch({
      # First check if the file is readable and complete
      json_text <- readLines(file, warn = FALSE)
      if (length(json_text) == 0) {
        warning(paste("Empty JSON file:", file, "- skipping"))
        next
      }
      
      # Validate JSON structure
      #if (!grepl("\\}\\s*$", paste(tail(json_text, 1), collapse=""))) {
      #  warning(paste("Incomplete JSON file:", file, "- skipping"))
      #  next
      #
      
      # Parse the JSON
      jsonlite::fromJSON(file)
    }, error = function(e) {
      warning(paste("Error reading JSON file:", file, "- Error:", e$message))
      return(NULL)
    })
    
    # Skip this file if reading failed
    if (is.null(json_data)) {
      next
    }
    
    # Check for valid structure - handle both flat and nested formats
    keys <- names(unlist(json_data, recursive = FALSE))
    
    # Check for necessary data structures using a more flexible approach
    has_summary_structure <- any(grepl("^summary\\.", keys)) || "summary" %in% names(json_data)
    has_before_filtering <- any(grepl("before_filtering", keys))
    has_after_filtering <- any(grepl("after_filtering", keys))
    
    if (!all(c(has_summary_structure, has_before_filtering, has_after_filtering))) {
      warning(paste("JSON file has unexpected structure:", file, "- skipping"))
      next
    }
    
    # Calculate Q30 percentages with error handling
    q30_percentages <- tryCatch({
      calculate_q30_percentage(json_data)
    }, error = function(e) {
      warning(paste("Error calculating Q30 for file:", file, "- Error:", e$message))
      return(NULL)
    })
    
    if (is.null(q30_percentages)) {
      next
    }
    
    # Determine if this is paired-end data by checking for read1/read2 fields
    is_paired <- any(grepl("read1_after_filtering", keys)) && any(grepl("read2_after_filtering", keys))
    
    if (is_paired) {
      # Handle paired-end data, safely extract values
      # This will work for both nested and flat structures
      read1_q30_after <- tryCatch({
        if ("read1_after_filtering" %in% names(json_data)) {
          # Flat structure
          json_data$read1_after_filtering$q30_bases / json_data$read1_after_filtering$total_bases * 100
        } else {
          # Might be nested differently
          r1_after <- json_data[[grep("read1_after_filtering", names(json_data), value = TRUE)]]
          r1_after$q30_bases / r1_after$total_bases * 100
        }
      }, error = function(e) {
        warning(paste("Error extracting read1_after_filtering data:", e$message))
        return(NA)
      })
      
      read2_q30_after <- tryCatch({
        if ("read2_after_filtering" %in% names(json_data)) {
          # Flat structure
          json_data$read2_after_filtering$q30_bases / json_data$read2_after_filtering$total_bases * 100
        } else {
          # Might be nested differently
          r2_after <- json_data[[grep("read2_after_filtering", names(json_data), value = TRUE)]]
          r2_after$q30_bases / r2_after$total_bases * 100
        }
      }, error = function(e) {
        warning(paste("Error extracting read2_after_filtering data:", e$message))
        return(NA)
      })
      
      # Similarly for before filtering data
      read1_q30_before <- tryCatch({
        if ("read1_before_filtering" %in% names(json_data)) {
          json_data$read1_before_filtering$q30_bases / json_data$read1_before_filtering$total_bases * 100
        } else {
          r1_before <- json_data[[grep("read1_before_filtering", names(json_data), value = TRUE)]]
          r1_before$q30_bases / r1_before$total_bases * 100
        }
      }, error = function(e) {
        warning(paste("Error extracting read1_before_filtering data:", e$message))
        return(NA)
      })
      
      read2_q30_before <- tryCatch({
        if ("read2_before_filtering" %in% names(json_data)) {
          json_data$read2_before_filtering$q30_bases / json_data$read2_before_filtering$total_bases * 100
        } else {
          r2_before <- json_data[[grep("read2_before_filtering", names(json_data), value = TRUE)]]
          r2_before$q30_bases / r2_before$total_bases * 100
        }
      }, error = function(e) {
        warning(paste("Error extracting read2_before_filtering data:", e$message))
        return(NA)
      })
      
      # Calculate differences
      diff_before <- abs(read1_q30_before - read2_q30_before)
      diff_after <- abs(read1_q30_after - read2_q30_after)
      
      # Check thresholds
      passes_diff_threshold <- 
        !is.na(diff_before) && !is.na(diff_after) &&
        diff_before <= q30_params$forward_reverse_q30_diff_cutoff * 100 &&
        diff_after <= q30_params$forward_reverse_q30_diff_cutoff * 100
      
      passes_q30_threshold <- 
        !is.na(read1_q30_after) && !is.na(read2_q30_after) &&
        (read1_q30_after >= q30_params$q30_cutoff * 100) &&
        (read2_q30_after >= q30_params$q30_cutoff * 100)
    } else {
      # Handle single-end data
      read1_q30_after <- NA
      read2_q30_after <- NA
      diff_before <- NA
      diff_after <- NA
      passes_diff_threshold <- TRUE  # Not applicable for single-end
      passes_q30_threshold <- q30_percentages$after_filtering >= q30_params$q30_cutoff * 100
    }
    
    # Add to results dataframe
    results <- rbind(results, data.frame(
      sample_name = sample_name,
      q30_before = q30_percentages$before_filtering,
      q30_after = q30_percentages$after_filtering,
      read1_q30_after = read1_q30_after,
      read2_q30_after = read2_q30_after,
      diff_r1_r2_before = diff_before,
      diff_r1_r2_after = diff_after,
      passes_q30_threshold = passes_q30_threshold,
      passes_diff_threshold = passes_diff_threshold,
      is_paired_end = is_paired,
      stringsAsFactors = FALSE
    ))
  }
  
  # Check if we processed any files successfully
  if (nrow(results) == 0) {
    warning("No valid JSON files were successfully processed.")
    return(NULL)
  }
  
  message(paste("Successfully processed", nrow(results), "of", length(json_files), "JSON files."))
  return(results)
}

plot_q30_scores <- function(results) {
  if (is.null(results) || nrow(results) == 0) {
    print("No Q30 scores data available to plot.")
    return()
  }
  # Separate single-end and paired-end data
  single_end_data <- results %>% dplyr::filter(!is_paired_end)
  paired_end_data <- results %>% dplyr::filter(is_paired_end)
  
  # Plot single-end data
  if (nrow(single_end_data) > 0) {
    p_single <-
      ggplot(
        single_end_data,
        aes(x = sample_name, y = q30_after, fill = "Percentage of Q30 or higher bases")
      ) +
      geom_bar(stat = "identity") +
      geom_hline(yintercept = 70,
                 linetype = "dashed",
                 color = "red") +
      scale_y_continuous(limits = c(0, 100), expand = c(0, 0)) +
      labs(x = "Sample Name", y = "Q30 Percentage", fill = "") +
      theme_minimal() +
      theme(
        legend.position = "top",
        axis.text.x = element_text(
          angle = 90,
          hjust = 1,
          vjust = 1
        )
      ) +
      geom_text(
        aes(
          x = sample_name,
          y = q30_after / 2,
          label = paste(round(q30_after, 2))
        ),
        vjust = 0.5,
        angle = 90
      )
    print(p_single)
    ggsave(p_single,
           file = paste0(paths$output, "/Q30_plot_single_end_reads.pdf"))
    #include_safe_graphics(paste0(paths$output, "/Q30_plot_single_end_reads.pdf"))
  }
  
  # Plot paired-end data
  if (nrow(paired_end_data) > 0) {
    paired_end_data_long <- paired_end_data %>%
      dplyr::select(sample_name,
             read1_q30_after,
             read2_q30_after,
             diff_r1_r2_after) %>%
      tidyr::pivot_longer(
        cols = c("read1_q30_after", "read2_q30_after"),
        names_to = "read",
        values_to = "q30_after"
      ) %>%
      dplyr::mutate(read = stringr::str_split(read, pattern = "_q30_", simplify = TRUE)[, 1])
    
    p_paired <-
      ggplot(
        paired_end_data_long,
        aes(
          x = interaction(read, sample_name),
          y = q30_after,
          fill = "Percentage of Q30 or higher bases"
        )
      ) +
      geom_bar(stat = "identity", position = "dodge") +
      geom_hline(yintercept = 70,
                 linetype = "dashed",
                 color = "red") +
      scale_y_continuous(limits = c(0, 100), expand = c(0, 0)) +
      labs(x = "Sample Name (Read)", y = "Q30 Percentage", fill = "") +
      theme_minimal() +
      theme(
        legend.position = "top",
        axis.text.x = element_text(
          angle = 90,
          hjust = 1,
          vjust = 1
        )
      ) +
      geom_text(
        aes(y = q30_after / 2, label = paste(round(q30_after, 2))),
        vjust = 0.5,
        angle = 90,
        position = position_dodge(width = 0.9)
      ) +
      geom_text(
        aes(
          x = interaction(read, sample_name),
          y = 90,
          label = paste0("Diff: ", round(diff_r1_r2_after, 2))
        ),
        position = position_dodge2(width = 0.9, padding = 0.5),
        angle = 90
      )
    
    print(p_paired)
    ggsave(p_paired,
           file = paste0(paths$output, "/Q30_plot_paired_end_reads.pdf"))
    #include_safe_graphics(paste0(paths$output, "/Q30_plot_paired_end_reads.pdf"))
  }
}

#params <- list(q30_cutoff = 0.7, forward_reverse_q30_diff_cutoff = 0.25)
q30_params <-
  list(q30_cutoff = q30_cutoff,
       forward_reverse_q30_diff_cutoff = forward_reverse_q30_diff_cutoff)
json_dir <- paste0(paths$output, "/fastp/")
results_df <- parse_fastp_json(json_dir, q30_params)
if (!is.null(results_df)) {
  #print(results_df)
  knitr::kable(results_df,
               caption = "Q30 Results") %>%
    kable_styling(
      bootstrap_options = "striped",
      full_width = F,
      position = "left"
    ) %>%
    kableExtra::scroll_box(width = "100%", height = "480px")
  plot_q30_scores(results_df)
}

is_paired <- results_df$is_paired_end[1]

```

## Read alignment percentage

A read alignment percentage cutoff of `r align_threshold*100`% is used to separate outliers.

```{r Percent_Mapping_filtering, echo=FALSE, eval=TRUE, warning=FALSE, fig.height=8, fig.width=10, out.width='100%', out.height='100%'}

# Load necessary libraries
library(tidyverse)

# Set alignment threshold
alignment_threshold <- align_threshold
nmr_threshold <- nmr_threshold

# Function to add comma separators
add_commas <- function(x) {
  format(x, big.mark = ",", scientific = FALSE)
}

# Determine the file path based on the alignment/quantification method
align_quant_method <- Sys.getenv("ALIGN_QUANT_METHOD")

# Initialize qc_path
qc_path <- NULL

if (align_quant_method == "star_salmon") {
  # Check if the directory exists
  multiqc_dir <- file.path(paths$reports, "multiqc_data")
  if (!dir.exists(multiqc_dir)) {
    print("Did not find the QC output from the STAR tool. The multiqc_report_data directory is missing. Please review.")
  } else {
    print("MultiQC found the QC output from the STAR tool.")
    qc_path <- normalizePath(file.path(multiqc_dir, "multiqc_star.txt"), mustWork = FALSE)
  }
} else if (align_quant_method == "star_rsem") {
  # Check if the directory exists
  multiqc_dir <- file.path(paths$reports, "multiqc_data")
  if (!dir.exists(multiqc_dir)) {
    print("Did not find the QC output from the RSEM tool. The multiqc_report_data directory is missing. Please review.")
  } else {
    print("MultiQC found the QC output from the RSEM tool.")
    qc_path <- normalizePath(file.path(multiqc_dir, "samtools_alignment_plot.txt"), mustWork = FALSE)
  }
} else if (align_quant_method == "salmon") {
  # Check if the directory exists
  salmon_dir <- file.path(paths$output, align_quant_method)
  if (!dir.exists(salmon_dir)) {
    print("Did not find the QC output from the Salmon tool.")
  } else {
    print("Found the QC output from the Salmon tool.")
    qc_path <- normalizePath(file.path(salmon_dir, "unique_multimapped_reads.txt"), mustWork = FALSE)
  }
} else if (align_quant_method == "kallisto") {
  # Check if the directory exists
  kallisto_dir <- file.path(paths$output, align_quant_method)
  if (!dir.exists(kallisto_dir)) {
    print("Did not find the QC output from the Kallisto tool.")
  } else {
    print("Found the QC output from the Kallisto tool.")
    qc_path <- normalizePath(file.path(kallisto_dir, "unique_multimapped_reads.txt"), mustWork = FALSE)
  }
} else {
  stop("Unsupported alignment method specified.")
}

# Check if the file exists
if (is.null(qc_path) || !file.exists(qc_path)) {
  stop(
    "Did not find the QC output file at ", qc_path, 
    ". Please review the multiqc directory or the pseudoalignment directory along with the calculate_unique_multimapped_reads_X.sh script."
  )
}

# Process data based on alignment method
if (align_quant_method %in% c("salmon", "kallisto")) {
  # Process salmon/kallisto file
  lines <- readLines(qc_path)
  summary_start <- grep("Summary Statistics:", lines)
  data_lines <- lines[1:(summary_start - 1)]
  temp_file <- tempfile()
  writeLines(data_lines, temp_file)
  data <- read.delim(temp_file, quote = "", header = TRUE, 
                     stringsAsFactors = FALSE, fill = TRUE, sep = "")
  unlink(temp_file)
  
  # Create plot data
  plot_data <- data %>%
    dplyr::select(Sample, Pct_Unique_of_Total, Pct_Multi_of_Total) %>%
    dplyr::rename(
      uniquely_mapped_percent = Pct_Unique_of_Total,
      multimapped_percent = Pct_Multi_of_Total
    ) %>%
    tidyr::pivot_longer(
      cols = c(uniquely_mapped_percent, multimapped_percent),
      names_to = "Mapping_Type",
      values_to = "Percentage"
    )
  
  #Create plot data for raw counts
  raw_data <- data
  long_counts <- data %>%
    dplyr::select(Sample, N_Unique.Reads, N_Multi.Reads) %>%
    dplyr::rename(uniquely_mapped = N_Unique.Reads, multimapped = N_Multi.Reads) %>%
    tidyr::pivot_longer(
      cols = c(uniquely_mapped, multimapped),
      names_to = "Mapping_Type",
      values_to = "Count"
    )

    #Create qc_data object for QAQC table
    align_map_qc_data <- data %>%
      dplyr::select(Sample, N_Total.Mapped, Pct_Mapped_of_Total) %>%
      dplyr::rename(total_mapped = N_Total.Mapped, total_mapped_percent = Pct_Mapped_of_Total) %>%
      dplyr::mutate(
        passed_alignment_threshold = if_else(
          condition = total_mapped_percent >= alignment_threshold * 100,
          true = TRUE,
          false = FALSE
        )
      ) %>%
      dplyr::arrange(desc(total_mapped_percent))

} else if (align_quant_method == "star_salmon") {
  # Process STAR-Salmon data
  starqc <- read_delim(file = qc_path, delim = "\t")
  starqc <- starqc %>%
    dplyr::mutate(
      total_mapped_percent = multimapped_percent + uniquely_mapped_percent,
      total_mapped = uniquely_mapped + multimapped
    ) %>%
    dplyr::mutate(
      passed_alignment_threshold = if_else(
        condition = total_mapped_percent >= alignment_threshold * 100,
        true = TRUE,
        false = FALSE
      )
    ) %>%
    dplyr::arrange(desc(total_mapped_percent))
  #Create qc_data object for QAQC table
  align_map_qc_data <- starqc

  # Create plot data
  plot_data <- starqc %>%
    dplyr::select(Sample, uniquely_mapped_percent, multimapped_percent) %>%
    tidyr::pivot_longer(
      cols = c(uniquely_mapped_percent, multimapped_percent),
      names_to = "Mapping_Type",
      values_to = "Percentage"
    )
  
  # Create plot data for raw counts
  long_counts <- starqc %>%
    dplyr::select(Sample, uniquely_mapped, multimapped) %>%
    tidyr::pivot_longer(
      cols = c(uniquely_mapped, multimapped),
      names_to = "Mapping_Type",
      values_to = "Count"
    )
} else if (align_quant_method == "star_rsem") {
  # Process STAR-RSEM data
  starqc <- read_delim(file = qc_path, delim = "\t")
  starqc <- starqc %>%
    dplyr::mutate(
      Total = `Mapped (with MQ>0)` + MQ0 + Unmapped,
      total_mapped = `Mapped (with MQ>0)` + MQ0,
      total_mapped_percent = (total_mapped / Total) * 100
    ) %>%
    dplyr::rename(
      Unique = `Mapped (with MQ>0)`,
      Multi = MQ0
    ) %>%
    dplyr::mutate(
      passed_alignment_threshold = if_else(
        condition = total_mapped_percent >= alignment_threshold * 100,
        true = TRUE,
        false = FALSE
      )
    ) %>%
    dplyr::arrange(desc(total_mapped_percent))
  #Create qc_data object for QAQC table
  align_map_qc_data <- starqc

  # Create plot data
  plot_data <- starqc %>%
    dplyr::select(Sample, Unique, Multi, Total) %>%
    dplyr::mutate(
      uniquely_mapped_percent = Unique / Total * 100,
      multimapped_percent = Multi / Total * 100
    ) %>%
    tidyr::pivot_longer(
      cols = c(uniquely_mapped_percent, multimapped_percent),
      names_to = "Mapping_Type",
      values_to = "Percentage"
    )
  
  #Create plot data for raw counts
  long_counts <- starqc %>%
    dplyr::select(Sample, Unique, Multi) %>%
    dplyr::rename(uniquely_mapped = Unique, multimapped = Multi) %>%
    tidyr::pivot_longer(
      cols = c(uniquely_mapped, multimapped),
      names_to = "Mapping_Type",
      values_to = "Count"
    )
}

# Create the stacked barplot for percentages
gg_percent_aligned <-
  ggplot(plot_data, aes(x = Sample, y = Percentage, fill = Mapping_Type)) +
  geom_bar(stat = "identity") +
  geom_hline(
    yintercept = alignment_threshold * 100,
    linetype = "dashed",
    color = "black"
  ) +  # Add horizontal line at threshold
  geom_text(
    data = plot_data %>% dplyr::filter(Mapping_Type == "uniquely_mapped_percent"),
    aes(y = Percentage / 2, label = sprintf("%.1f%%", Percentage)),
    color = "black",
    size = 3.5,
    angle = 90
  ) +
  geom_text(
    data = plot_data %>% dplyr::filter(Mapping_Type == "multimapped_percent"),
    aes(
      y = Percentage / 2 + (plot_data %>% dplyr::filter(Mapping_Type == "uniquely_mapped_percent") %>% dplyr::pull(Percentage)),
      label = sprintf("%.1f%%", Percentage)
    ),
    color = "black",
    size = 3.5,
    angle = 90
  ) +
  scale_y_continuous(
    limits = c(0, 100),
    breaks = seq(0, 100, by = 10),
    expand = c(0, 0)
  ) +
  labs(x = "Sample", y = "Percentage Aligned", fill = "Alignment Type") +
  scale_fill_manual(
    values = c(
      "uniquely_mapped_percent" = "yellow",
      "multimapped_percent" = "lightblue"
    ),
    labels = c(
      "uniquely_mapped_percent" = "Uniquely aligned",
      "multimapped_percent" = "Multiple alignments"
    )
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Print the plot
print(gg_percent_aligned)
#Save the plot
ggsave(
  gg_percent_aligned,
  file = paste0(paths$output, "/Percentage_Aligned_Threshold_Plot.pdf")
)

# Create the stacked barplot for raw counts
gg_raw_count_aligned <-
  ggplot(long_counts,
         aes(x = Sample, y = Count, fill = Mapping_Type)) +
  geom_bar(stat = "identity") +
  geom_hline(yintercept = nmr_threshold,
             linetype = "dashed",
             color = "black") +  # Add horizontal line at threshold
  geom_text(
    data = long_counts %>% dplyr::filter(Mapping_Type == "uniquely_mapped"),
    aes(y = Count / 2, label = add_commas(Count)),
    color = "black",
    size = 3.5,
    angle = 90
  ) +
  geom_text(
    data = long_counts %>% dplyr::filter(Mapping_Type == "multimapped"),
    aes(
      y = Count / 2 + (long_counts %>% 
                         dplyr::filter(Mapping_Type == "uniquely_mapped") %>% 
                         dplyr::pull(Count)),
      label = add_commas(Count)
    ),
    color = "black",
    size = 3.5,
    angle = 90
  ) +
  scale_y_continuous(
    labels = add_commas, 
    expand = c(0, 0.05)
  ) +
  labs(x = "Sample", y = "Count", fill = "Alignment Type") +
  scale_fill_manual(
    values = c(
      "uniquely_mapped" = "yellow",
      "multimapped" = "lightblue"
    ),
    labels = c("uniquely_mapped" = "Uniquely aligned", "multimapped" = "Multiple alignments")
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Print the plot
#print(gg_raw_count_aligned)

#Save the plot
ggsave(
  gg_raw_count_aligned,
  file = paste0(paths$output, "/Raw_Counts_Aligned_Threshold_Plot.pdf")
)
```

## Total reads aligned

An absolute number of `r nmr_threshold`, representing 10% of the target sequencing depth, is used to separate outliers.
Reasons for reads being alignable to the genome but not quantifiable include:
- Reads that are aligned to the genome but do not map to the transcriptome; reads that do not map to any known annotated transcripts (e.g., intergenic regions, retained introns, non-coding regions, unannotated exons or novel splice variants).
- Multi-mapping reads that map equally well to multiple transcripts in the transcriptome, and therefore cannot be assigned to a single transcript (i.e., are discarded or fractionally assigned. Possibly from gene fusion or overlapping gene annotations).
- Incomplete or low-quality non-model reference genome annotations that do not include all transcripts & splice variants, or incorrect gene boundaries causing ambiguous multi-mapping.
- Very short reads that may align to too many places in the genome, and thus can not be confidently assigned to a transcript.
```{r total_reads_mapped_plot, echo=FALSE, eval=TRUE, warning=FALSE, fig.height=8, fig.width=10, out.width='100%', out.height='100%'}
print(gg_raw_count_aligned)
```

## Gene quantification statistics

Reporting the number of reads that were successfully quantified (mapped to genes) and their percentage relative to aligned reads and total (filtered) reads.


```{r Gene_Quantification_Stats, echo=FALSE, eval=TRUE, warning=FALSE, fig.height=8, fig.width=10, out.width='100%', out.height='100%'}
# Calculate sum of counts for each sample from the gene count matrix
quantified_reads_per_sample <- colSums(sampleData)

# Create a data frame with the quantification stats
quant_stats <- data.frame(
  Sample = names(quantified_reads_per_sample),
  quantified_reads = quantified_reads_per_sample,
  stringsAsFactors = FALSE
)

# Clean up sample names if needed to match alignment data
# You may need to adjust this depending on your sample naming conventions
quant_stats$Sample <- gsub("^X", "", quant_stats$Sample)  # Remove X prefix if present
quant_stats$Sample <- gsub("\\.[0-9]+$", "", quant_stats$Sample)  # Remove file extensions if present
quant_stats$Sample <- gsub("_[12]$", "", quant_stats$Sample) # Then remove _1 or _2 at the end

# Join with alignment data
combined_stats <- align_map_qc_data %>%
  dplyr::left_join(quant_stats, by = "Sample") %>%
  dplyr::mutate(
    # Percentage of aligned reads that were quantified
    pct_quantified_of_aligned = (quantified_reads / total_mapped) * 100
  )

# Add percentage of total reads if your data has this information
# Load the fastp filtered reads data to get total read counts
fastp_path <- file.path(paths$reports, "multiqc_data", "fastp_filtered_reads_plot.txt")
if (file.exists(fastp_path)) {
  fastp_data <- read.delim(fastp_path, check.names = FALSE)
  
  # Calculate total reads (sum of all categories)
  # Using regular if statement for a single value
  if(is_paired) {
    fastp_data <- fastp_data %>%
      dplyr::mutate(`Passed Filter` = `Passed Filter` / 2) %>%
      dplyr::mutate(`Low Quality` = `Low Quality` / 2) %>%
      dplyr::mutate(`Too Many N` = `Too Many N` / 2) %>%
      dplyr::mutate(`Too Short` = `Too Short` / 2) %>%
      dplyr::mutate(total_raw_reads = `Passed Filter` + `Low Quality` + `Too Many N` + `Too Short`)
  } else {
    fastp_data <- fastp_data %>%
      dplyr::mutate(total_raw_reads = `Passed Filter` + `Low Quality` + `Too Many N` + `Too Short`)
  }
  # Make sure Sample column is character for joining
  fastp_data$Sample <- as.character(fastp_data$Sample)
} else {
  warning("Fastp filtered reads file not found at: ", fastp_path)
  fastp_data <- NULL
}

# Join with alignment data
combined_stats <- align_map_qc_data %>%
  dplyr::left_join(quant_stats, by = "Sample")

# Add total reads information if available
if (!is.null(fastp_data)) {
  combined_stats <- combined_stats %>%
    dplyr::left_join(fastp_data %>% select(Sample, `Passed Filter`, total_raw_reads), by = "Sample") %>%
    dplyr::mutate(
      # Percentage of aligned reads that were quantified
      pct_quantified_of_aligned = (quantified_reads / total_mapped) * 100,
      # Percentage of passed filter reads that were quantified
      pct_quantified_of_passed = (quantified_reads / `Passed Filter`) * 100,
      # Percentage of total raw reads that were quantified
      pct_quantified_of_total = (quantified_reads / total_raw_reads) * 100
    )
} else {
  combined_stats <- combined_stats %>%
    dplyr::mutate(
      # Percentage of aligned reads that were quantified
      pct_quantified_of_aligned = (quantified_reads / total_mapped) * 100
    )
  
  # Add percentage of total reads if your data has this information from other sources
  if ("Total" %in% colnames(align_map_qc_data) || exists("starqc") && "Total" %in% colnames(starqc)) {
    combined_stats <- combined_stats %>%
      dplyr::mutate(pct_quantified_of_total = (quantified_reads / Total) * 100)
  } else if (align_quant_method %in% c("salmon", "kallisto")) {
    combined_stats <- combined_stats %>%
      dplyr::left_join(data %>% select(Sample, N_All.Reads), by = "Sample") %>%
      dplyr::mutate(pct_quantified_of_total = (quantified_reads / N_All.Reads) * 100)
  }
}

# Create a plot showing quantification rates as percentage of aligned reads
gg_quantification <- ggplot(combined_stats, 
                           aes(x = Sample, y = pct_quantified_of_aligned)) +
  geom_bar(stat = "identity", fill = "darkgreen") +
  geom_text(aes(label = sprintf("%.1f%%", pct_quantified_of_aligned)),
            vjust = -0.5, size = 3.5) +
  scale_y_continuous(limits = c(0, 100), 
                     breaks = seq(0, 100, by = 10),
                     expand = c(0, 10)) +
  labs(x = "Sample", 
       y = "Percentage of Aligned Reads",
       title = "Percentage of Aligned Reads Mapped to Genes") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Create a plot showing quantification rates as percentage of total reads
if (!is.null(fastp_data)) {
  gg_total_quantification <- ggplot(combined_stats, 
                                   aes(x = Sample, y = pct_quantified_of_total)) +
    geom_bar(stat = "identity", fill = "steelblue") +
    geom_text(aes(label = sprintf("%.1f%%", pct_quantified_of_total)),
              vjust = -0.5, size = 3.5) +
    scale_y_continuous(limits = c(0, 100), 
                       breaks = seq(0, 100, by = 10),
                       expand = c(0, 0)) +
    labs(x = "Sample", 
         y = "Percentage of Total Raw Reads",
         title = "Percentage of Total Raw Reads Mapped to Genes") +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
}

# Create a plot showing raw quantified reads
gg_quantified_counts <- ggplot(combined_stats, 
                              aes(x = Sample, y = quantified_reads)) +
  geom_bar(stat = "identity", fill = "darkgreen") +
  geom_text(aes(label = add_commas(quantified_reads)),
            angle = 90, hjust = -0.1, size = 3.5) +
  labs(x = "Sample", 
       y = "Number of Reads",
       title = "Number of Reads Mapped to Genes") +
  scale_y_continuous(labels = add_commas, expand = c(0, 0.05)) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Load SortMeRNA data to get rRNA filtering information
sortmerna_path <- file.path(paths$reports, "multiqc_data", "multiqc_sortmerna.txt")
if (file.exists(sortmerna_path)) {
  sortmerna_data <- read.delim(sortmerna_path, check.names = FALSE)
  
  # Clean up the data (some samples may have empty rows)
  sortmerna_data <- sortmerna_data %>%
    filter(!is.na(total) & total > 0) %>%
    dplyr::mutate(
      Sample = gsub("^X", "", Sample),  # Remove X prefix if present
      Sample = gsub("\\.[0-9]+$", "", Sample),  # Remove file extensions
      Sample = gsub("_[12]$", "", Sample)  # Remove _1 or _2 at the end
    )
} else {
  warning("SortMeRNA file not found at: ", sortmerna_path)
  sortmerna_data <- NULL
}

# Modify the read journey plot to include rRNA filtering
if (!is.null(fastp_data) && !is.null(sortmerna_data)) {
  # Join the SortMeRNA data with our combined stats
  combined_stats <- combined_stats %>%
    dplyr::left_join(sortmerna_data %>% select(Sample, total, rRNA, non_rRNA), by = "Sample")
  
  # Adjust for paired-end reads if necessary
  if (is_paired) {
    # Double the alignment and quantification counts to match how other tools count reads
    combined_stats <- combined_stats %>%
      dplyr::mutate(
        total_mapped = total_mapped * 2,
        quantified_reads = quantified_reads * 2
      )
  }
  
  read_journey <- combined_stats %>%
    dplyr::mutate(
      reads_filtered_out = total_raw_reads - `Passed Filter`,
      rRNA_filtered = rRNA,  # New category for rRNA reads
      passed_not_aligned = non_rRNA - total_mapped,  # Adjust calculation to account for rRNA filtering
      aligned_not_quantified = total_mapped - quantified_reads
    ) %>%
    dplyr::select(Sample, total_raw_reads, reads_filtered_out, rRNA_filtered, passed_not_aligned, aligned_not_quantified, quantified_reads) %>%
    tidyr::pivot_longer(
      cols = c(reads_filtered_out, rRNA_filtered, passed_not_aligned, aligned_not_quantified, quantified_reads),
      names_to = "read_category",
      values_to = "count"
    ) %>%
    dplyr::mutate(read_category = factor(read_category, 
                                  levels = c("reads_filtered_out", "rRNA_filtered", "passed_not_aligned", 
                                            "aligned_not_quantified", "quantified_reads")))
  
  # Rest of code remains the same
  category_labels <- c(
    "reads_filtered_out" = "Failed QC Filter", 
    "rRNA_filtered" = "rRNA Removed",
    "passed_not_aligned" = "Passed QC & rRNA Filter But Not Aligned", 
    "aligned_not_quantified" = "Aligned But Not Quantified", 
    "quantified_reads" = "Quantified"
  )
  
  gg_read_journey <- ggplot(read_journey, aes(x = Sample, y = count, fill = read_category)) +
    geom_bar(stat = "identity") +
    scale_fill_manual(
      values = c(
        "reads_filtered_out" = "lightgray",
        "rRNA_filtered" = "salmon",
        "passed_not_aligned" = "lightpink",
        "aligned_not_quantified" = "lightyellow",
        "quantified_reads" = "darkgreen"
      ),
      labels = category_labels
    ) +
    labs(x = "Sample", 
         y = "Number of Reads",
         fill = "Read Category",
         title = "Read Journey from Raw Sequencing to Quantification") +
    scale_y_continuous(labels = add_commas) +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1),
          legend.position = "bottom")
}

# Print the plots
print(gg_quantification)

# Print total reads quantification plot if available
if (exists("gg_total_quantification")) {
  print(gg_total_quantification)
}

print(gg_quantified_counts)

# Print read journey plot if available
if (exists("gg_read_journey")) {
  print(gg_read_journey)
}

# Save the plots
ggsave(
  gg_quantification,
  file = paste0(paths$output, "/Percentage_Quantified_Plot.pdf")
)

ggsave(
  gg_quantified_counts,
  file = paste0(paths$output, "/Raw_Counts_Quantified_Plot.pdf")
)

# Save additional plots if available
if (exists("gg_total_quantification")) {
  ggsave(
    gg_total_quantification,
    file = paste0(paths$output, "/Percentage_Total_Quantified_Plot.pdf")
  )
}

if (exists("gg_read_journey")) {
  ggsave(
    gg_read_journey,
    file = paste0(paths$output, "/Read_Journey_Plot.pdf"),
    width = 12, height = 8
  )
}
```

## Dendrogram, all samples

A tree height cutoff of `r tree_height_cutoff` is used to cut the tree and separate outliers.

Technical control samples are coloured differently.

```{r split_metadata}
############################################################################
# Dendrogram: split into experimental samples vs technical controls
############################################################################

tech_ctrl_names <- DESeqDesign %>%
  dplyr::filter(!!ensym(technical_control) == T) %>%
  dplyr::pull(original_names)

sample_names <- DESeqDesign %>%
  dplyr::filter(!!ensym(technical_control) == F) %>%
  dplyr::pull(original_names)

ref_samples <- DESeqDesign %>%
  dplyr::filter(!!ensym(reference_rna) == T) %>%
  dplyr::pull(original_names)

```

```{r dendrogram_clustering, echo = FALSE, fig.width = 11, fig.height = 10, out.height='100%', out.width='100%'}



############################################################################
# Function for sorting dendrograms
############################################################################
sort_hclust <- function(...)
  as.hclust(dendsort(as.dendrogram(...)))

############################################################################
# Re-order the data by group
############################################################################
flag_order <- order(d[[treatment_var]])
d <- d[flag_order, ]
sampleData <- sampleData[, flag_order]

############################################################################
# CPM (used in dendrogram/cluster analysis)
############################################################################
libsize <- apply(sampleData, 2, sum)
cpm <- sampleData
for (k in 1:length(libsize)) {
  cpm[, k] <- log2((10 ^ 6) * (sampleData[, k] + 0.5) / (libsize[k] + 1))
}

############################################################################
# Dendrogram: all
############################################################################
CairoPDF(
  file = normalizePath(file.path(
    paths$output, "dendrogram_prefiltering_all.pdf"
  )),
  width = 14,
  height = 8.5,
  family = "Ubuntu"
)

dendro_before_all_samples <-
  1 - cor(as.matrix(cpm), method = clust_method)
dendro_before_all_samples <-
  sort_hclust(hclust(as.dist(dendro_before_all_samples), method = "average"))

dendro_before_all_samples <-
  as.dendrogram(dendro_before_all_samples)
colors_to_use <-
  as.numeric(as.factor(DESeqDesign[, technical_control]))
ordered_colors <-
  colors_to_use[order.dendrogram(dendro_before_all_samples)]
labels_colors(dendro_before_all_samples) <- ordered_colors

original_names <- DESeqDesign[, "original_names"]
original_names <-
  original_names[order.dendrogram(dendro_before_all_samples)]

labels_to_use <- DESeqDesign[, dendro_color_by]
labels_to_use <-
  labels_to_use[order.dendrogram(dendro_before_all_samples)]
labels_to_use <- paste(labels_to_use, original_names)

plot(
  dendro_before_all_samples %>% dendextend::set("labels", labels_to_use),
  main = "log2 CPM",
  horiz = F
)
abline(h = tree_height_cutoff, col = "red", lwd = 2)
dev.off()
```

## Dendrogram, technical controls only

```{r dendro_tech_before, echo = FALSE, fig.width = 11, fig.height = 10, out.height='100%', out.width='100%'}
############################################################################
# Technical Controls, Prefiltering
############################################################################
dendro_before_tech_ctrls <-
  1 - cor(as.matrix(cpm %>% dplyr::select(all_of(tech_ctrl_names))),
          method = "spearman")

if (length(dendro_before_tech_ctrls) > 2) {
  DESeqDesignTechCtrls <-
    DESeqDesign %>% dplyr::filter(original_names %in% all_of(tech_ctrl_names))
  
  dendro_before_tech_ctrls <-
    sort_hclust(hclust(as.dist(dendro_before_tech_ctrls), method = "average"))
  
  dendro_before_tech_ctrls <-
    as.dendrogram(dendro_before_tech_ctrls)
  colors_to_use <-
    as.numeric(as.factor(DESeqDesignTechCtrls[, dendro_color_by]))
  ordered_colors <-
    colors_to_use[order.dendrogram(dendro_before_tech_ctrls)]
  labels_colors(dendro_before_tech_ctrls) <- ordered_colors
  
  original_names <- DESeqDesignTechCtrls[, "original_names"]
  original_names <-
    original_names[order.dendrogram(dendro_before_tech_ctrls)]
  
  labels_to_use <- DESeqDesignTechCtrls[, dendro_color_by]
  labels_to_use <-
    labels_to_use[order.dendrogram(dendro_before_tech_ctrls)]
  labels_to_use <- paste(labels_to_use, original_names)
  
  CairoPDF(
    file = normalizePath(
      file.path(paths$output,
                "dendrogram_prefiltering_tech_controls.pdf")
    ),
    width = 14,
    height = 8.5,
    family = "Ubuntu"
  )
  
  plot(
    dendro_before_tech_ctrls %>% dendextend::set("labels", labels_to_use),
    main = "log2 CPM",
    horiz = F
  )
  abline(h = tree_height_cutoff, col = "red", lwd = 2)
  
  # Create legend
  unique_labels <- unique(DESeqDesignTechCtrls[, dendro_color_by])
  unique_colors <- unique(colors_to_use)
  
  legend("topright",
         legend = unique_labels,
         fill = unique_colors,
         title = dendro_color_by)
  
  dev.off()
}

include_safe_graphics(file.path(paths$output, "dendrogram_prefiltering_tech_controls.pdf"))

```

## Part one - Dendrogram, experimental samples only 

Coloured by `r treatment_var`.

```{r dendro_samples_before, echo = FALSE, fig.width = 11, fig.height = 10, out.height='100%', out.width='100%'}
############################################################################
# Experimental Samples, Prefiltering
############################################################################

CairoPDF(
  file = normalizePath(
    file.path(paths$output,
              "dendrogram_prefiltering_exp_samples.pdf")
  ),
  width = 14,
  height = 8.5,
  family = "Ubuntu"
)

dendro_before_all_exp_samples <-
  1 - cor(as.matrix(cpm %>% dplyr::select(all_of(sample_names))),
          method = "spearman")

DESeqDesignExpSamples <-
  DESeqDesign %>% dplyr::filter(original_names %in% all_of(sample_names))

dendro_before_all_exp_samples <-
  sort_hclust(hclust(as.dist(dendro_before_all_exp_samples), method = "average"))

dendro_before_all_exp_samples <-
  as.dendrogram(dendro_before_all_exp_samples)
colors_to_use <-
  as.numeric(as.factor(DESeqDesignExpSamples[, treatment_var]))
ordered_colors <-
  colors_to_use[order.dendrogram(dendro_before_all_exp_samples)]
labels_colors(dendro_before_all_exp_samples) <- ordered_colors

original_names <- DESeqDesignExpSamples[, "original_names"]
original_names <-
  original_names[order.dendrogram(dendro_before_all_exp_samples)]

labels_to_use <- DESeqDesignExpSamples[, dendro_color_by]
labels_to_use <-
  labels_to_use[order.dendrogram(dendro_before_all_exp_samples)]
labels_to_use <- paste(labels_to_use, original_names)

plot(
  dendro_before_all_exp_samples %>% dendextend::set("labels", labels_to_use),
  main = "log2 CPM",
  horiz = F
)
abline(h = tree_height_cutoff, col = "red", lwd = 2)

# Create legend
unique_labels <- unique(DESeqDesignExpSamples[, treatment_var])
unique_colors <- unique(colors_to_use)

legend("topright",
       legend = unique_labels,
       fill = unique_colors,
       title = treatment_var)

dev.off()

#include_safe_graphics(file.path(paths$output, "dendrogram_prefiltering_exp_samples.pdf"))


```

## Part two - Dendrogram, experimental samples only

Coloured by `r dendro_color_by`.

The dose coloured dendrogram is hard-coded in this version and should be removed if your experiment does not involve dose-response (and therefore won't have a dose column in the metadata).  

```{r dendro_experimental_samples_by_dose, echo = FALSE, fig.width = 11, fig.height = 10, out.height='100%', out.width='100%'}
############################################################################
# Experimental Samples, Prefiltering, Colored by Dose
############################################################################

CairoPDF(
  file = normalizePath(
    file.path(
      paths$output,
      "dendrogram_prefiltering_exp_samples_by_dose.pdf"
    )
  ),
  width = 14,
  height = 8.5,
  family = "Ubuntu"
)

dendro_before_all_exp_samples <-
  1 - cor(as.matrix(cpm %>% dplyr::select(all_of(sample_names))),
          method = "spearman")

DESeqDesignExpSamples <-
  DESeqDesign %>% dplyr::filter(original_names %in% all_of(sample_names))

dendro_before_all_exp_samples <-
  sort_hclust(hclust(as.dist(dendro_before_all_exp_samples), method = "average"))

dendro_before_all_exp_samples <-
  as.dendrogram(dendro_before_all_exp_samples)
colors_to_use <-
  as.numeric(as.factor(DESeqDesignExpSamples[, dendro_color_by]))
ordered_colors <-
  colors_to_use[order.dendrogram(dendro_before_all_exp_samples)]
labels_colors(dendro_before_all_exp_samples) <- ordered_colors

original_names <- DESeqDesignExpSamples[, "original_names"]
original_names <-
  original_names[order.dendrogram(dendro_before_all_exp_samples)]

labels_to_use <- DESeqDesignExpSamples[, dendro_color_by]
labels_to_use <-
  labels_to_use[order.dendrogram(dendro_before_all_exp_samples)]
labels_to_use <- paste(labels_to_use, original_names)

plot(
  dendro_before_all_exp_samples %>% dendextend::set("labels", labels_to_use),
  main = "log2 CPM",
  horiz = F
)
abline(h = tree_height_cutoff, col = "red", lwd = 2)

# Create legend
unique_labels <- unique(DESeqDesignExpSamples[, dendro_color_by])
unique_colors <- unique(colors_to_use)

legend("topright",
       legend = unique_labels,
       fill = unique_colors,
       title = dendro_color_by)

dev.off()

#include_safe_graphics(file.path(paths$output, "dendrogram_prefiltering_exp_samples_by_dose.pdf"))
```

## PCA

```{r generate_PCA_1, echo = FALSE, fig.width = 11, fig.height = 10, out.height='100%', out.width='100%'}
# Create PCA
require(DESeq2)
require(ggplot2)

# Row Variance - taken from genefilter package
rowVars <- function (x, ...) {
  sqr = function(x)
    x * x
  n = rowSums(!is.na(x))
  n[n <= 1] = NA
  return(rowSums(sqr(x - rowMeans(x, ...)), ...) / (n - 1))
}

# Round up to nearest 10, 100, etc
RoundUp <- function(from, to)
  ceiling(from / to) * to

# Variance Stabilization Function - from DESeq package
varStab <-
  function (object,
            blind = TRUE,
            nsub,
            fitType = "parametric") {
    if (nrow(object) < nsub) {
      stop(
        "less than 'nsub' rows,\n  it is recommended to use varianceStabilizingTransformation directly"
      )
    }
    if (is.null(colnames(object))) {
      colnames(object) <- seq_len(ncol(object))
    }
    if (is.matrix(object)) {
      matrixIn <- TRUE
      object <-
        DESeqDataSetFromMatrix(object, DataFrame(row.names = colnames(object)),
                               ~ 1)
    }
    else {
      if (blind) {
        design(object) <- ~ 1
      }
      matrixIn <- FALSE
    }
    if (is.null(sizeFactors(object)) &
        is.null(normalizationFactors(object))) {
      object <- estimateSizeFactors(object)
    }
    baseMean <- rowMeans(counts(object, normalized = TRUE))
    if (sum(baseMean > 5) < nsub) {
      stop(
        "less than 'nsub' rows with mean normalized count > 5, \n  it is recommended to use varianceStabilizingTransformation directly"
      )
    }
    object.sub <- object[baseMean > 5,]
    baseMean <- baseMean[baseMean > 5]
    o <- order(baseMean)
    idx <- o[round(seq(
      from = 1,
      to = length(o),
      length = nsub
    ))]
    object.sub <- object.sub[idx,]
    object.sub <- estimateDispersionsGeneEst(object.sub, quiet = TRUE)
    object.sub <-
      estimateDispersionsFit(object.sub, fitType = fitType,
                             quiet = TRUE)
    suppressMessages({
      dispersionFunction(object) <- dispersionFunction(object.sub)
    })
    vsd <- varianceStabilizingTransformation(object, blind = FALSE)
    if (matrixIn) {
      return(assay(vsd))
    }
    else {
      return(vsd)
    }
  }

# PCA pre-process function
pca_fun <- function(x, condition) {
  round(x) # In case counts are non-integer value
  # Create column data
  coldata <- data.frame(row.names = colnames(x), condition)
  # Make DESeqDataSetFromMatrix
  dds <-
    DESeqDataSetFromMatrix(countData = x,
                           colData = coldata,
                           design = ~ condition)
  return(dds)
}


# Define count table
ct <- sampleData # This is the count table object
b_frame <- as.data.frame(ct)
# Process data for PCA plot
condition <- factor(c(colnames(b_frame)))
b_frame_rounded <- round(b_frame)
f_dds <- pca_fun(b_frame_rounded, condition)
ntop <- 500
nsub <- RoundUp(ncol(b_frame_rounded) * 0.90, 1)
vsd <- varStab(f_dds, nsub = nsub)
rv <- rowVars(assay(vsd))
select = order(rv, decreasing = TRUE)[seq_len(min(ntop, length(rv)))]
pca = prcomp(t(assay(vsd)[select,]))
scores <- data.frame(condition, pca$x)

# Calculate the percentage of variance explained by PC1 and PC2
percent_var <- round(100 * (pca$sdev ^ 2 / sum(pca$sdev ^ 2)), 1)
pc1_label <- paste0("PC1 (", percent_var[1], "%)")
pc2_label <- paste0("PC2 (", percent_var[2], "%)")

# Plot PCA and output to PDF
pdf(file = paste0(paths$output, "/PCA.pdf"))
# Plot PC1 vs. PC2
pca_p <- ggplot(scores, aes(x = PC1, y = PC2, color = condition)) +
  geom_point(size = 3) +
  xlab(pc1_label) +
  ylab(pc2_label) +
  labs(color = 'Samples') +
  theme_minimal()
print(pca_p)
dev.off()
cat("PCA Completed\n")

#include_safe_graphics(paste0(paths$output, "/PCA.pdf"))
print(pca_p)
```

## PCA by treatment and dose

```{r generate_PCA_2_by_dose, echo = FALSE, eval = !is.null(params$dose), fig.width = 11, fig.height = 10, out.height='100%', out.width='100%'}

##PCA by Dose##

# Row Variance - taken from genefilter package
rowVars <- function (x, ...) {
  sqr = function(x)
    x * x
  n = rowSums(!is.na(x))
  n[n <= 1] = NA
  return(rowSums(sqr(x - rowMeans(x, ...)), ...) / (n - 1))
}

# Round up to nearest 10, 100, etc
RoundUp <- function(from, to)
  ceiling(from / to) * to

# Variance Stabilization Function - from DESeq package
varStab <-
  function (object,
            blind = TRUE,
            nsub,
            fitType = "parametric") {
    if (nrow(object) < nsub) {
      stop(
        "less than 'nsub' rows,\n  it is recommended to use varianceStabilizingTransformation directly"
      )
    }
    if (is.null(colnames(object))) {
      colnames(object) <- seq_len(ncol(object))
    }
    if (is.matrix(object)) {
      matrixIn <- TRUE
      object <-
        DESeqDataSetFromMatrix(object, DataFrame(row.names = colnames(object)),
                               ~ 1)
    } else {
      if (blind) {
        design(object) <- ~ 1
      }
      matrixIn <- FALSE
    }
    if (is.null(sizeFactors(object)) &
        is.null(normalizationFactors(object))) {
      object <- estimateSizeFactors(object)
    }
    baseMean <- rowMeans(counts(object, normalized = TRUE))
    if (sum(baseMean > 5) < nsub) {
      stop(
        "less than 'nsub' rows with mean normalized count > 5, \n  it is recommended to use varianceStabilizingTransformation directly"
      )
    }
    object.sub <- object[baseMean > 5,]
    baseMean <- baseMean[baseMean > 5]
    o <- order(baseMean)
    idx <- o[round(seq(
      from = 1,
      to = length(o),
      length = nsub
    ))]
    object.sub <- object.sub[idx,]
    object.sub <- estimateDispersionsGeneEst(object.sub, quiet = TRUE)
    object.sub <-
      estimateDispersionsFit(object.sub, fitType = fitType,
                             quiet = TRUE)
    suppressMessages({
      dispersionFunction(object) <- dispersionFunction(object.sub)
    })
    vsd <- varianceStabilizingTransformation(object, blind = FALSE)
    if (matrixIn) {
      return(assay(vsd))
    } else {
      return(vsd)
    }
  }

# PCA pre-process function
pca_fun <- function(x, condition) {
  round(x) # In case counts are non-integer value
  # Create column data
  coldata <- data.frame(row.names = colnames(x), condition)
  # Make DESeqDataSetFromMatrix
  dds <-
    DESeqDataSetFromMatrix(countData = x,
                           colData = coldata,
                           design = ~ condition)
  return(dds)
}

# Process data for PCA plot
condition <- factor(c(colnames(b_frame)))
b_frame_rounded <- round(b_frame)
f_dds <- pca_fun(b_frame_rounded, condition)
ntop <- 500
nsub <- RoundUp(ncol(b_frame_rounded) * 0.90, 1)
vsd <- varStab(f_dds, nsub = nsub)
rv <- rowVars(assay(vsd))
select = order(rv, decreasing = TRUE)[seq_len(min(ntop, length(rv)))]
pca = prcomp(t(assay(vsd)[select,]))

#Assuming the first column in "d" is the sample names
colnames(d)[1] <- "Run"

scores <- data.frame(Run = rownames(pca$x), pca$x)
dose_col <- ensym(dose)
compound_col <- ensym(treatment_var)
meta_data <- d %>%
  dplyr::select("Run",!!dose_col,!!compound_col)
meta_data[[as.character(dose_col)]] <-
  as.factor(meta_data[[as.character(dose_col)]]) #set as factor
meta_data[[as.character(compound_col)]] <-
  as.factor(meta_data[[as.character(compound_col)]]) #set as factor

# Merge with metadata for coloring
scores <- merge(scores, meta_data, by.x = "Run", by.y = "Run")

# Calculate the percentage of variance explained by PC1 and PC2
percent_var <- round(100 * (pca$sdev ^ 2 / sum(pca$sdev ^ 2)), 1)
pc1_label <- paste0("PC1 (", percent_var[1], "%)")
pc2_label <- paste0("PC2 (", percent_var[2], "%)")

# Plot PCA and output to PDF
pdf(file = paste0(paths$output, "/PCA_dose.pdf"))
# Plot PC1 vs. PC2
pca_p <-
  ggplot(scores,
         aes(
           x = PC1,
           y = PC2,
           color = !!dose_col,
           label = Run,
           shape = !!compound_col
         )) +
  geom_point(size = 3) +
  geom_text(vjust = -0.5, hjust = 1) +
  xlab(pc1_label) +
  ylab(pc2_label) +
  labs(color = 'Dose', shape = 'Compound') +
  theme_minimal()
print(pca_p)
dev.off()
cat("PCA Colored by Dose Completed\n")

#include_safe_graphics(paste0(paths$output, "/PCA_dose.pdf"))
print(pca_p)

filter_pca_variance <- function(scores, threshold = 20) {
  pca_filter_results_list <- NULL
  for (k in unique(scores[[treatment_var]])) {
    scores_filt <- scores %>%
      dplyr::rename(Sample = Run) %>%
      dplyr::filter(!!sym(treatment_var) == k)
    dose_groups <- unique(scores_filt[[dose]])
    pca_filter_results <- data.frame(
      Sample = character(),
      PC1_var = numeric(),
      PC2_var = numeric(),
      Status = character(),
      stringsAsFactors = FALSE
    )
    pca_filter_results[[dose]] <- character()
    pca_filter_results[[treatment_var]] <- character()
    
    
    for (i in dose_groups) {
      dose_samples <- scores_filt[scores_filt[[dose]] == i,]
      if (nrow(dose_samples) > 2) {
        pc1_median <- median(dose_samples$PC1)
        pc2_median <- median(dose_samples$PC2)
        
        # Calculate the variance percentage
        dose_samples$PC1_var <- abs(dose_samples$PC1 - pc1_median)
        dose_samples$PC2_var <- abs(dose_samples$PC2 - pc2_median)
        
        # Determine pass/fail status
        dose_samples$Status <-
          ifelse(
            dose_samples$PC1_var > threshold |
              dose_samples$PC2_var > threshold,
            "FAIL",
            "PASS"
          )
        
        # Append to the results data frame
        pca_filter_results <-
          rbind(pca_filter_results, dose_samples[, c("Sample",
                                                     dose,
                                                     treatment_var,
                                                     "PC1_var",
                                                     "PC2_var",
                                                     "Status")])
      } else {
        print(
          "Your experimental design does not include enough replicates... skipping filtering. Your samples will automatically PASS and you will have to review the PCA plot yourself and decide if you want to remove samples."
        )
        
        dose_samples$PC1_var <- dose_samples$PC1
        dose_samples$PC2_var <- dose_samples$PC1
        
        # Determine pass/fail status
        dose_samples$Status <- "PASS"
        
        # Append to the results data frame
        pca_filter_results <-
          rbind(pca_filter_results, dose_samples[, c("Sample",
                                                     dose,
                                                     treatment_var,
                                                     "PC1_var",
                                                     "PC2_var",
                                                     "Status")])
      }
    }
    pca_filter_results_list[[k]] <- pca_filter_results
  }
  # Combine all results into a single data frame
  combined_results <- bind_rows(pca_filter_results_list)
  
  return(combined_results)
}

# After the PCA plot generation, use the filtering function
pca_filter_results <-
  filter_pca_variance(scores, threshold = PCA_cutoff * 100)
```

## Number of active probes (NCov5/NCovn)

The proportion of probes having >5 uniquely mapped reads are calculated. Outliers on that metric (i.e., outside 3XIQR) are removed from analysis.

```{r NCov5_boxplot, echo = FALSE, fig.width = 11, fig.height = 10, out.height='100%', out.width='100%'}
######################################################################################
# Ncov5 The number of probes with at least 5 uniquely mapped reads.
######################################################################################
# Tukey's Outer Fence cutoff - 3*IQR
Ncov5 <- data.frame(Sample = names(sampleData))
n_genes <- nrow(sampleData)
Ncov5$value <-
  apply(sampleData, 2, function(x)
    length(x[x > 4])) / n_genes
percent_active_probes <- boxplot(Ncov5$value, range = 3, plot = F)
failed_Ncov5 <-
  Ncov5[Ncov5$value < percent_active_probes$stats[1, 1] |
          Ncov5$value > percent_active_probes$stats[5, 1],]

# Determine the y-axis limits to accommodate all points, including outliers if they exist
ylim_range <- range(Ncov5$value)
if (nrow(failed_Ncov5) > 0) {
  ylim_range <- range(c(Ncov5$value, failed_Ncov5$value))
}

# Create the base plot
boxplot(
  Ncov5$value,
  range = 3,
  plot = TRUE,
  ylab = "Proportion of probes with >5 uniquely mapped reads",
  main = "NCov5",
  xlab = "All samples",
  outline = FALSE,
  ylim = ylim_range
)

# Jitter x-coordinates for all points
jittered_x <- jitter(rep(1, length(Ncov5$value)), amount = 0.2)

# Add jittered points, coloring the outliers in red
points(
  jittered_x,
  Ncov5$value,
  col = ifelse(
    Ncov5$value < percent_active_probes$stats[1, 1] |
      Ncov5$value > percent_active_probes$stats[5, 1],
    "red",
    "black"
  ),
  pch = 16
)

# Add sample names to the outliers, if any exist, using the same jittered x-coordinates
if (nrow(failed_Ncov5) > 0) {
  outlier_indices <- which(
    Ncov5$value < percent_active_probes$stats[1, 1] |
      Ncov5$value > percent_active_probes$stats[5, 1]
  )
  
  text(
    jittered_x[outlier_indices],
    Ncov5$value[outlier_indices],
    labels = Ncov5$Sample[outlier_indices],
    pos = 4,
    col = "red"
  )
}
```

## Number of probes capturing 80% of signal (NSig80)

The number of probes it takes to capture 80% of the signal is first calculated, and samples falling outside of 3X IQR are removed from downstream analysis.

```{r NSig80_boxplot, echo = FALSE, fig.width = 11, fig.height = 10, out.height='100%', out.width='100%'}
######################################################################################
# Nsig80 - The number of probes capturing the top 80% of signal in a sample.
######################################################################################
# Tukey's Outer Fence cutoff - 3*IQR
# (include test samples, vehicle controls, and reference chemical treatments)

### Using some of the EPA code for now:
countStats <- function(counts,
                       nsig = getOption("httrStatNsig", default = 0.8)) {
  # Load required pkgs
  require(foreach)
  # Store results in a vector
  st <- vector()
  # Drop probes w/ no reads (this is for efficiency, should not impact final Nsig values)
  counts_nz <- counts[counts > 0]
  # Sort in decreasing order
  counts_nz <- sort(counts_nz, decreasing = T)
  # Compute cumulative sum of counts starting from highest count probe
  cumCounts <- cumsum(counts_nz)
  # Convert cumulative sums to cumulative proportions
  cumProp <- cumCounts / sum(counts_nz)
  # Compute the minimum number of probes to capture X% of total reads
  if (length(nsig) > 0) {
    ns <-
      foreach(prop = nsig, .combine = 'c') %do% {
        min(which(cumProp > prop))
      }
    names(ns) <- paste0("n_sig", nsig * 100)
    st <- append(st, ns)
  }
  return(st)
}

# ref <- apply(sampleData/QAQC$NMR, 2, median)
# sig80 <- quantile(ref, prob = 0.8)
#
# QAQC$Nsig80 <- 0
#
# for(k in 1:nrow(QAQC)){
#   flag80pct <- sampleData[,k] > sig80 * QAQC$NMR[k]
#   QAQC$Nsig80[k] <- length(sampleData[flag80pct, 1])/n_samples
# }
#

# Create a data frame for Nsig80 values
Nsig80 <- data.frame(Sample = names(sampleData))

# Calculate the number of probes it takes to capture 80% of the signal
Nsig80$value <- apply(sampleData, 2, countStats)

# Generate boxplot stats without plotting
probes_capturing_top_80 <-
  boxplot(Nsig80$value, range = 3, plot = FALSE)

# Identify outliers based on the 3x IQR rule
failed_Nsig80 <-
  Nsig80[Nsig80$value < probes_capturing_top_80$stats[1, 1] |
           Nsig80$value > probes_capturing_top_80$stats[5, 1], ]

# Determine the y-axis limits to accommodate all points, including outliers if they exist
ylim_range <- range(Nsig80$value)
if (nrow(failed_Nsig80) > 0) {
  ylim_range <- range(c(Nsig80$value, failed_Nsig80$value))
}

# Create the base boxplot with dynamic y-axis limits
boxplot(
  Nsig80$value,
  range = 3,
  plot = TRUE,
  ylab = "# of probes it takes to capture 80% of the signal",
  main = "Nsig80",
  xlab = "All samples",
  outline = FALSE,
  ylim = ylim_range
)

# Jitter x-coordinates for all points
jittered_x <- jitter(rep(1, length(Nsig80$value)), amount = 0.2)

# Add jittered points, coloring the outliers in red
points(
  jittered_x,
  Nsig80$value,
  col = ifelse(
    Nsig80$value < probes_capturing_top_80$stats[1, 1] |
      Nsig80$value > probes_capturing_top_80$stats[5, 1],
    "red",
    "black"
  ),
  pch = 16
)

# Add sample names to the outliers, if any exist, using the same jittered x-coordinates
if (nrow(failed_Nsig80) > 0) {
  outlier_indices <- which(
    Nsig80$value < probes_capturing_top_80$stats[1, 1] |
      Nsig80$value > probes_capturing_top_80$stats[5, 1]
  )
  
  text(
    jittered_x[outlier_indices],
    Nsig80$value[outlier_indices],
    labels = Nsig80$Sample[outlier_indices],
    pos = 4,
    col = "red"
  )
}
```

## Gini coefficient

The Gini coefficient is computed for each sample based on the distribution of raw counts for all probes including those with 0 aligned reads. Samples with Gini coefficients >`r gini_cutoff` are removed from the analysis.

```{r gini_base_plot, echo = FALSE, fig.width = 11, fig.height = 10, out.height='100%', out.width='100%'}
######################################################################################
# Gini coefficient - Gini coefficient computed for each sample based on the distribution of
# raw counts for all probes including those with 0 aligned reads
######################################################################################
# Define the Gini cutoff threshold
#gini_cutoff <- 0.95 #Default

# Create a data frame for Gini coefficients
Gini <- data.frame(Sample = names(sampleData))

# Calculate the Gini coefficient for each sample
Gini$value <- apply(sampleData, 2, function(x)
  gini(x))

# Identify samples with Gini values exceeding the cutoff
failed_gini <- Gini[Gini$value > gini_cutoff,]

# Determine the y-axis limits to accommodate all points, and extend upwards to 1
ylim_range <- range(Gini$value)
ylim_range[2] <- 1  # Set the upper limit of the y-axis to 1

# Create a boxplot of the Gini coefficients with dynamic y-axis limits
boxplot(
  Gini$value,
  main = "Gini Coefficients",
  ylab = "Gini Coefficient",
  xlab = "All samples",
  outline = FALSE,
  ylim = ylim_range
)



# Jitter x-coordinates for all points
jittered_x <- jitter(rep(1, length(Gini$value)), amount = 0.2)

# Add jittered points, coloring the outliers in red
points(
  jittered_x,
  Gini$value,
  col = ifelse(Gini$value > gini_cutoff, "red", "black"),
  pch = 16
)

# Add sample names to the outliers, if any exist, using the same jittered x-coordinates
if (nrow(failed_gini) > 0) {
  outlier_indices <- which(Gini$value > gini_cutoff)
  
  text(
    jittered_x[outlier_indices],
    Gini$value[outlier_indices],
    labels = Gini$Sample[outlier_indices],
    pos = 4,
    col = "red"
  )
}
```

## Additional misc. QC metrics

```{r generate_additional_QC_figures_1, echo = FALSE, fig.width = 11, fig.height = 10, out.height='100%', out.width='100%'}
#Read counts table from command line
ct <- sampleData # This is the count table object
if (!is.null(mu)) {
  mu_t <- t(mu) #This is the mapped-unmapped file from TempO-Seq
} else {
  print(
    "WARNING: No Mapped-unmapped file located. If your data is TempoSeq, please review. If not, proceeding..."
  )
}
#Create Dendrogram
b_frame <- as.data.frame(ct)
new_frame <- apply(b_frame, 2, as.numeric)
dat <- as.matrix(new_frame)
cd <- dist(t(dat))
cc <- hclust(cd)
hcd = as.dendrogram(cc)

pdf(file = paste0(paths$output, "/dendrogram.pdf"))
par(cex = 0.8)
plot(hcd, main = "Euclidean distance cluster dendrogram", ylab = "Height")
dev.off()
cat("Dendrogram Completed\n")

#include_safe_graphics(paste0(paths$output, "/dendrogram.pdf"))
par(cex = 0.8)
plot(hcd, main = "Euclidean distance cluster dendrogram", ylab = "Height")

#If mapped_unmapped table is present - create bar plot
if (!is.null(mu)) {
  #mu <- read.table(mu, sep=",", header=T, row.names=1, stringsAsFactors = F, check.names = F)
  
  #Generate bar plot of total reads/sample and the mapped and unmapped portion.
  ll2 <- as.matrix(mu_t)
  
  # Define the threshold
  threshold <- params$nmr_threshold
  
  mettol <- seq(1, dim(ll2)[2], 50)
  pdf(file = paste0(paths$output, "/barplot_mapped_unmapped.pdf"))
  par(cex = 1.5)
  par(las = 2)
  
  for (i in 1:length(mettol)) {
    if (dim(ll2)[2] >= (mettol[i] + 49)) {
      barplot(
        as.matrix(ll2[, mettol[i]:(mettol[i] + 49)]),
        col = c("yellow", "orange"),
        ylab = "read counts",
        xlab = "sample name",
        cex.axis = 0.5,
        cex.names = 0.5
      )
      abline(
        h = threshold,
        col = "red",
        lwd = 2,
        lty = 2
      )
      legend(
        "topleft",
        c("unmapped", "mapped", "10% target read depth"),
        fill = c("yellow", "orange", "red"),
        cex = 0.7
      )
    } else{
      barplot(
        as.matrix(ll2[, mettol[i]:dim(ll2)[2]]),
        col = c("yellow", "orange"),
        ylab = "read counts",
        xlab = "sample name",
        cex.axis = 0.5,
        cex.names = 0.5
      )
      abline(
        h = threshold,
        col = "red",
        lwd = 2,
        lty = 2
      )
      legend(
        "topleft",
        c("unmapped", "mapped", "10% target read depth"),
        fill = c("yellow", "orange", "red"),
        cex = 0.7
      )
    }
  }
  dev.off()
  cat("Bar plot Completed\n")
  
  include_safe_graphics(paste0(paths$output, "/barplot_mapped_unmapped.pdf"))
}
```

# {-}

# Sample filtering summary {.tabset}

## Q30 scores (percentage of bases over Q30 Score)

With a threshold cutoff of `r q30_cutoff*100`%

```{r q30_QAQC}
######################################################################################
#Percentage of bases over Q30 Score
#And percentage difference of Q30 score between read1 and read2 for paired data
######################################################################################
# DEFAULT: Reject < 0.7
results_df <- results_df %>%
  dplyr::rename(Sample = sample_name)

QAQC <-
  data.frame(Sample = names(sampleData)) #Column names of count matrix/sampleData are the sample names

QAQC <- QAQC %>%
  dplyr::left_join(results_df %>% dplyr::select(Sample, q30_after), by = "Sample") %>%
  dplyr::rename(q30 = q30_after)
failed_q30 <- QAQC[QAQC$q30 < q30_cutoff * 100, ]

# Check and print flagged samples
if (nrow(failed_q30) > 0) {
  print("Samples Flagged:")
  flagged_samples <- d %>%
    dplyr::filter(original_names %in% failed_q30$Sample) %>%
    dplyr::pull(original_names)
  print(flagged_samples)
} else {
  print("No samples failed the Q30 threshold.")
}
```

## Q30 scores - paired data only (percentage difference of Q30 score between read1 and read2)

With a threshold cutoff of `r align_threshold*100`% difference between read1 and read2. Note: Only applicable for paired-end read data.

```{r q30_difference_between_Read1_read2_QAQC}
######################################################################################
#Percentage of bases over Q30 Score
#And percentage difference of Q30 score between read1 and read2 for paired data
######################################################################################
# DEFAULT: Reject > 0.25

QAQC <- QAQC %>%
  dplyr::left_join(results_df %>% dplyr::select(Sample, diff_r1_r2_after), by = "Sample") %>%
  dplyr::rename(diff_r1_r2_q30 = diff_r1_r2_after)
failed_diff_q30 <-
  QAQC[QAQC$diff_r1_r2_q30 > forward_reverse_q30_diff_cutoff * 100, ]

# Check and print flagged samples
if (nrow(failed_diff_q30) > 0) {
  print("Samples Flagged:")
  flagged_samples <- d %>%
    dplyr::filter(original_names %in% failed_diff_q30$Sample) %>%
    dplyr::pull(original_names)
  print(flagged_samples)
} else {
  print("No samples failed the Q30 difference threshold.")
}
```

## Percent alignment

With a threshold cutoff of `r align_threshold*100`%

```{r alignment_QAQC}
######################################################################################
# Percentage Alignment
######################################################################################
# Add align_map_qc_data to QAQC object
QAQC <- QAQC %>%
  dplyr::left_join(align_map_qc_data %>% dplyr::select(Sample, total_mapped_percent), by = "Sample") %>%
  dplyr::rename(pct_mapped = total_mapped_percent)
failed_alignment <- QAQC[QAQC$pct_mapped < align_threshold * 100, ]

# Check and print flagged samples
if (nrow(failed_alignment) > 0) {
  print("Samples Flagged:")
  flagged_samples <- d %>%
    dplyr::filter(original_names %in% failed_alignment$Sample) %>%
    dplyr::pull(original_names)
  print(flagged_samples)
} else {
  print("No samples failed the alignment threshold.")
}
```

## Total mapped reads

Samples less than `r nmr_threshold` aligned reads removed.

```{r num_mapped_reads_QAQC}
######################################################################################
# Additional QC Metrics
######################################################################################
# NMR Number of mapped reads, defined as sum of total read counts summed over all detected probes
######################################################################################
# Reject < 300000 or Threshold = 10% of target depth
# Removes various types of failed samples

# Add NMR (Number of Mapped Reads) to the existing QAQC object
QAQC <- QAQC %>%
  dplyr::left_join(
    data.frame(Sample = names(sampleData), 
               NMR = apply(sampleData, 2, sum)), 
    by = "Sample"
  )
failed_read_threshold <- QAQC[QAQC$NMR < nmr_threshold, ]

# Check if any samples failed the NMR threshold and print them
if (nrow(failed_read_threshold) > 0) {
  print("Samples Flagged:")
  flagged_samples <- d %>%
    dplyr::filter(original_names %in% failed_read_threshold$Sample) %>%
    dplyr::pull(original_names)
  print(flagged_samples)
} else {
  print("No samples failed the NMR threshold.")
}

```

## Pearson correlation distance (dendrogram)

In the code below, a `r clust_method` distance of `r tree_height_cutoff` is used to cut the tree and separate outliers. This is a sample clustering technique.

```{r filtering_dendrogram_QAQC}
############################################################################
# Filtering
############################################################################

tree_groups <-
  cutree(dendro_before_all_samples, h = tree_height_cutoff)
n <- table(tree_groups)
n <- n[n == 1]

print("Samples Flagged:")

if (length(n) > 0) {  # Ensure the length of n is greater than 0
  flag_cluster <- d$original_names %in% names(tree_groups[tree_groups %in% as.integer(names(n))])
  
  # Write the flagged samples to a file
  write.table(
    d[flag_cluster, ],
    normalizePath(
      file.path(paths$processed, "outliers_cluster_analysis.txt")
    ),
    sep = "\t",
    row.names = FALSE,
    col.names = TRUE,
    quote = FALSE
  )
  
  # Print the group assignments for flagged samples
  print(d$group[flag_cluster])
} else {
  print("No samples were flagged in the clustering analysis.")
}
```

## Dose group clustering variance (from PCA)

With a threshold cutoff of `r PCA_cutoff*100`%

```{r PCA_variance_QAQC}
######################################################################################
# PCA Variance
######################################################################################
# Add pca_filter_results to QAQC object
QAQC <- QAQC %>%
  dplyr::left_join(pca_filter_results %>% dplyr::select(Sample, PC1_var, PC2_var), by = "Sample")
failed_PC1 <- QAQC[QAQC$PC1_var > PCA_cutoff * 100, ]
failed_PC2 <- QAQC[QAQC$PC2_var > PCA_cutoff * 100, ]

# Check if any samples failed the PCA variance threshold
print("Samples Flagged:")
flagged_samples <- d %>%
  dplyr::filter(original_names %in% failed_PC1$Sample | original_names %in% failed_PC2$Sample) %>%
  dplyr::pull(original_names)

if (length(flagged_samples) > 0) {
  print(flagged_samples)
} else {
  print("No samples failed the PCA variance thresholds.")
}
```

## Number of active probes (NCovn)

The number of probes having >5 uniquely mapped reads are calculated. Outliers on that metric (i.e., outside 3XIQR) are removed from analysis.

```{r ncov5_QAQC}
######################################################################################
# Ncov5 The number of probes with at least 5 uniquely mapped reads.
######################################################################################
# Tukey's Outer Fence cutoff - 3*IQR
n_genes <- nrow(sampleData)
QAQC$Ncov5 <-
  apply(sampleData, 2, function(x)
    length(x[x > 4])) / n_genes
percent_active_probes <- boxplot(QAQC$Ncov5, range = 3, plot = F)
failed_Ncov5 <-
  QAQC[QAQC$Ncov5 < percent_active_probes$stats[1, 1] |
         QAQC$Ncov5 > percent_active_probes$stats[5, 1], ]

# Check if any samples failed the Ncov5 threshold
print("Samples Flagged:")
flagged_samples <- d %>%
  dplyr::filter(original_names %in% failed_Ncov5$Sample) %>%
  dplyr::pull(original_names)

if (length(flagged_samples) > 0) {
  print(flagged_samples)
} else {
  print("No samples failed the Ncov5 threshold.")
}
```

## Number of probes capturing 80% of signal (NSig80)

The number of probes capturing 80% of the signal is first calculated, and samples falling outside of 3X IQR are removed from downstream analysis.

```{r nsig80_QAQC}
######################################################################################
# Nsig80 - The number of probes capturing the top 80% of signal in a sample.
######################################################################################
# Tukey's Outer Fence cutoff - 3*IQR
# (include test samples, vehicle controls, and reference chemical treatments)

QAQC$Nsig80 <- apply(sampleData, 2, countStats)

probes_capturing_top_80 <- boxplot(QAQC$Nsig80, range = 3, plot = F)
failed_Nsig80 <-
  QAQC[QAQC$Nsig80 < probes_capturing_top_80$stats[1, 1] |
         QAQC$Nsig80 > probes_capturing_top_80$stats[5, 1], ]

# Check if any samples failed the Nsig80 threshold
print("Samples Flagged:")
flagged_samples <- d %>%
  dplyr::filter(original_names %in% failed_Nsig80$Sample) %>%
  dplyr::pull(original_names)

if (length(flagged_samples) > 0) {
  print(flagged_samples)
} else {
  print("No samples failed the Nsig80 threshold.")
}

```

## Gini coefficient

The Gini coefficient is computed for each sample based on the distribution of raw counts for all probes including those with 0 aligned reads. Samples with Gini coefficients >`r gini_cutoff` are removed from the analysis.

```{r gini_QAQC}
######################################################################################
# Gini coefficient - Gini coefficient computed for each sample based on the distribution of
# raw counts for all probes including those with 0 aligned reads
######################################################################################
# DEFAULT: Reject > 0.95
QAQC$Gini <- apply(sampleData, 2, function(x)
  gini(x))
failed_gini <- QAQC[QAQC$Gini > gini_cutoff, ]

# Check if any samples failed the Gini threshold
print("Samples Flagged:")
flagged_samples <- d %>%
  dplyr::filter(original_names %in% failed_gini$Sample) %>%
  dplyr::pull(original_names)

if (length(flagged_samples) > 0) {
  print(flagged_samples)
} else {
  print("No samples failed the Gini coefficient threshold.")
}
```

# {-}

# Sample-to-sample correlations {.tabset}

## All samples

This shows the Pearson correlation of log2-normalized CPM of read counts across all samples.

```{r correlation_1, fig.width = 11, fig.height = 10, out.height='100%', out.width='100%'}
#From header: , fig.width = 11, fig.height = 10
ref_correlations_all <- cor(as.matrix(cpm),
                            method = "pearson")

correlation_df_all <- DESeqDesign %>%
  dplyr::filter(original_names %in% colnames(ref_correlations_all))

rownames(correlation_df_all) <- correlation_df_all$original_names
correlation_df_all <-
  correlation_df_all[colnames(ref_correlations_all), ]

correlation_df_all_heatmap <-
  correlation_df_all[, c(unlist(groups[[1]]),
                         technical_control,
                         reference_rna,
                         solvent_control)]

correlation_df_all_heatmap$technical_control <-
  as.numeric(correlation_df_all_heatmap$technical_control)
correlation_df_all_heatmap$reference_rna <-
  as.numeric(correlation_df_all_heatmap$reference_rna)
correlation_df_all_heatmap$solvent_control <-
  as.numeric(correlation_df_all_heatmap$solvent_control)
correlation_df_all_heatmap$Compound <-
  as.factor(correlation_df_all_heatmap$Compound)

pheatmap(
  ref_correlations_all,
  show_rownames = T,
  show_colnames = T,
  border_color = NA,
  annotation_col = correlation_df_all_heatmap,
  filename = file.path(paths$output, "Heatmap_Pearson_log2CPM_all_samples.pdf")
)

include_safe_graphics(file.path(paths$output, "Heatmap_Pearson_log2CPM_all_samples.pdf"))

```

## Experimental samples only

This shows the Pearson correlation of log2-normalized CPM of read counts across experimental samples.  **Technical controls are excluded.**

This first plot shows every sample in the study:

```{r correlation_2, fig.width = 11, fig.height = 10, out.height='100%', out.width='100%'}
ref_correlations_samples <- cor(as.matrix(cpm %>%
                                            dplyr::select(all_of(sample_names))),
                                method = "pearson")

correlation_df_samples <- DESeqDesign %>%
  dplyr::filter(original_names %in% colnames(ref_correlations_samples))

rownames(correlation_df_samples) <-
  correlation_df_samples$original_names
correlation_df_samples <-
  correlation_df_samples[colnames(ref_correlations_samples), ]

correlation_df_samples_heatmap <-
  correlation_df_samples[, c(unlist(groups[[1]]),
                             technical_control,
                             reference_rna,
                             solvent_control)]

correlation_df_samples_heatmap$technical_control <-
  as.numeric(correlation_df_samples_heatmap$technical_control)
correlation_df_samples_heatmap$reference_rna <-
  as.numeric(correlation_df_samples_heatmap$reference_rna)
correlation_df_samples_heatmap$solvent_control <-
  as.numeric(correlation_df_samples_heatmap$solvent_control)
correlation_df_samples_heatmap$Compound <-
  as.factor(correlation_df_samples_heatmap$Compound)

pheatmap(
  ref_correlations_samples,
  show_rownames = T,
  show_colnames = T,
  border_color = NA,
  annotation_col = correlation_df_samples_heatmap,
  filename = file.path(
    paths$output,
    "Heatmap_Pearson_log2CPM_experimental_samples.pdf"
  )
)

include_safe_graphics(file.path(
  paths$output,
  "Heatmap_Pearson_log2CPM_experimental_samples.pdf"
))

```

# {-}

## cellWise PCA outlier map {.tabset}

These plots show correlations within each experimental grouping. Faceted by the treatment variable (e.g., Chemical/Compound)

```{r PCA_outlier_map, fig.width = 11, fig.height = 10, out.height='100%', out.width='100%', echo = FALSE, results='asis'}
### To double check a group of interest... Generate and include PCA outlier maps in tabsets
facets <- unique(DESeqDesignExpSamples[, treatment_var])
cor_list <- list()

for (i in seq_along(facets)) {
  samples_in_facet <- DESeqDesignExpSamples[DESeqDesignExpSamples[[treatment_var]] == facets[i], "original_names"]
  cpm_subset <- as.matrix(cpm %>% dplyr::select(all_of(samples_in_facet)))
  pca_subset <- rrcov::PcaGrid(t(cpm_subset))
  
  # Generate the PCA outlier map and save as a unique PDF file
  file_name <- paste0("OutlierMap_", facets[i], "_Pearson_log2CPM_experimental_samples.pdf")
  pdf(file = file.path(paths$output, file_name))
  cellWise::outlierMap(pca_subset)
  dev.off()
  
  # Create a tab for each facet and include the corresponding PCA outlier map
  cat("### ", facets[i], "\n\n")
  cat("![", facets[i], "](", file.path(paths$output, file_name), ")\n\n")
}
```

## {-}

## Correlation Heatmap for sample groups {.tabset}

These plots show correlations within each experimental grouping. Faceted by the treatment variable (e.g., Chemical/Compound)

```{r Correlation_Heatmap, fig.width = 11, fig.height = 10, out.height='100%', out.width='100%', echo=FALSE, results='asis'}
# Generate correlation heatmaps and include them in tabsets
facets <- unique(DESeqDesignExpSamples[, treatment_var])
cor_list <- list()

for (i in seq_along(facets)) {
  samples_in_facet <- DESeqDesignExpSamples[DESeqDesignExpSamples[[treatment_var]] == facets[i], "original_names"]
  cpm_subset <- as.matrix(cpm %>% dplyr::select(all_of(samples_in_facet)))
  
  # Calculate correlations and prepare for heatmap
  correlations <- cor(cpm_subset, method = "pearson")
  correlation_df <- DESeqDesign %>%
    dplyr::filter(original_names %in% colnames(correlations))
  
  row.names(correlation_df) <- correlation_df$original_names
  correlation_df <- correlation_df[colnames(correlations), ]
  correlation_df <- as.data.frame(correlation_df[unlist(groups[[1]][1])])
  
  # Generate the correlation heatmap and save as a unique PDF file
  file_name <- paste0("Group_Heatmap_", facets[i], "_Pearson_log2CPM_experimental_samples.pdf")
  pheatmap(
    correlations,
    annotation_col = correlation_df,
    display_numbers = TRUE,
    cutree_cols = 2,
    cutree_rows = 2,
    filename = file.path(paths$output, file_name)
  )
  
  # Create a tab for each facet and include the corresponding heatmap
  cat("### ", facets[i], "\n\n")
  cat("![", facets[i], "](", file.path(paths$output, file_name), ")\n\n")
  
  # Store correlation data for further analysis
  correlations <- as.data.frame(correlations)
  correlations$original_names <- row.names(correlations)
  cor_distribution <- correlations %>%
    dplyr::mutate(mean = rowMeans(across(where(is.numeric)))) %>%
    dplyr::select(mean, original_names) %>%
    dplyr::mutate(facet = facets[i])
  cor_list[[i]] <- cor_distribution
}
```

## {-}

# Distribution of Sample-to-sample correlations {.tabset}

## Mean of per-sample correlations with every other sample in the study

In other words, for each sample, what is its mean correlation with every other sample?

```{r correlations_4, fig.width = 11, fig.height = 10, out.height='100%', out.width='100%'}
# Create the correlation data frame
cor_df1 <- as.data.frame(ref_correlations_samples)
cor_df1$original_names <- row.names(cor_df1)

# Calculate the study-wide correlation means
cor_distribution_all <- cor_df1 %>%
  dplyr::mutate(mean = rowMeans(across(where(is.numeric)))) %>%
  dplyr::select(mean, original_names)

# Set dynamic lower limit for y-axis
ymin <- min(cor_distribution_all$mean, na.rm = TRUE)

# Create the plot with dynamic y-axis limits and minimal theme
gg <- ggplot(cor_distribution_all, aes(x = "", y = mean)) +
  geom_boxplot(outlier.shape = NA) +
  geom_jitter(position = position_jitter(seed = 1, width = 0.2)) +
  geom_text(
    aes(label = original_names),
    position = position_jitter(seed = 1, width = 0.2),
    hjust = 0,
    vjust = 0,
    check_overlap = TRUE
  ) +
  ylim(ymin, 1) +
  ylab("Mean Pearson Correlation Coefficient") +
  xlab("") +  # Empty x-axis label since it's study-wide
  theme_minimal()

# Save the plot
ggsave(gg,
       file = paste0(paths$output, "/Samp_for_samp_corr_comparison_boxplot.pdf"))

# Print the plot
print(gg)
```

## Mean of per-sample correlations with every other sample in its corresponding group

In other words, for each sample, what is its mean correlation with every other sample **within in its experimental grouping**?

```{r correlations_5, fig.width = 11, fig.height = 10, out.height='100%', out.width='100%'}
# MEANS BY CHEMICAL
cor_means_by_chem <- data.table::rbindlist(cor_list)
# Set dynamic lower limit for y-axis
ymin <- min(cor_means_by_chem$mean, na.rm = TRUE)

# Create the plot with dynamic y-axis limits, minimal theme, and meaningful labels
gg <- ggplot(cor_means_by_chem, aes(facet, mean)) +
  geom_boxplot(outlier.shape = NA) +
  geom_jitter(position = position_jitter(seed = 1, width = 0.2)) +
  geom_text(
    aes(label = original_names),
    position = position_jitter(seed = 1, width = 0.2),
    hjust = 0,
    vjust = 0,
    check_overlap = TRUE
  ) +
  ylim(ymin, 1) +
  ylab(paste0("Mean Pearson Correlation Coefficient by ", treatment_var)) +
  xlab(treatment_var) +
  theme_minimal()

# Save the plot
ggsave(
  gg,
  file = paste0(
    paths$output,
    "/Samp_for_samp_corr_by_chemical_comparison_boxplot.pdf"
  )
)

# Print the plot
print(gg)
```

## Mean of per-sample correlations with every other sample within the same dose group

In other words, for each sample, what is its mean correlation with every other sample **within its dose group**?

```{r correlations_within_dose_group_6, fig.width = 11, fig.height = 10, out.height='100%', out.width='100%'}
# Assuming 'dose_var' is the variable representing the dose group
dose_var <- dose

# Initialize list for storing correlation distributions
cor_list_by_group <- list()

# Iterate over each treatment and dose combination
for (treatment in unique(DESeqDesignExpSamples[[treatment_var]])) {
  for (dose_n in unique(DESeqDesignExpSamples[[dose_var]])) {
    # Get samples in the current treatment and dose_n group
    samples_in_group <- DESeqDesignExpSamples[DESeqDesignExpSamples[[treatment_var]] == treatment &
                                                DESeqDesignExpSamples[[dose_var]] == dose_n, "original_names"]
    
    # Check the number of samples in the group
    if (length(samples_in_group) == 1) {
      message(paste(
        "Skipping group",
        treatment,
        dose_n,
        "because it has only 1 sample."
      ))
      cor_list_by_group[[paste(treatment, dose_n, sep = "_")]] <-
        list()
      next
    } else if (length(samples_in_group) == 2) {
      message(
        paste(
          "Warning: Group",
          treatment,
          dose_n,
          "has only 2 samples. Correlation calculation is not meaningful."
        )
      )
      cor_list_by_group[[paste(treatment, dose_n, sep = "_")]] <-
        list()
      next
    }
    
    # Proceed with correlation calculation if 3 or more samples
    if (length(samples_in_group) >= 3) {
      # Subset CPM data for the current group
      cpm_subset <-
        as.matrix(cpm %>% dplyr::select(all_of(samples_in_group)))
      
      # Calculate correlations for the current group
      correlations <- cor(cpm_subset, method = "pearson")
      correlation_df <- DESeqDesign %>%
        dplyr::filter(original_names %in% colnames(correlations))
      
      row.names(correlation_df) <- correlation_df$original_names
      correlation_df <- correlation_df[colnames(correlations), ]
      
      # Create a data frame for the current group
      correlations <- as.data.frame(correlations)
      correlations$original_names <- row.names(correlations)
      cor_distribution <- correlations %>%
        dplyr::mutate(mean = rowMeans(across(where(is.numeric)))) %>%
        dplyr::select(mean, original_names) %>%
        dplyr::mutate(treatment = treatment, dose_n = dose_n)
      
      # Add the current group's data to the list
      cor_list_by_group[[paste(treatment, dose_n, sep = "_")]] <-
        cor_distribution
    }
  }
}

if (length(cor_list_by_group) > 0) {
  # Combine the list into a single data frame
  cor_means_by_group <- data.table::rbindlist(cor_list_by_group)
} else {
  cor_means_by_group <- data.frame()
}

# Check if the data frame is empty
if (!exists("cor_means_by_group") ||
    nrow(cor_means_by_group) == 0) {
  message("All groups have only 1 or 2 samples. No correlation plot can be generated.")
} else {
  # Set dynamic lower limit for y-axis
  ymin <- min(cor_means_by_group$mean, na.rm = TRUE)
  
  # Create the plot with dynamic y-axis limits, minimal theme, and meaningful labels
  gg <-
    ggplot(cor_means_by_group, aes(x = interaction(treatment, dose_n), y = mean)) +
    geom_boxplot(outlier.shape = NA) +
    geom_jitter(position = position_jitter(seed = 1, width = 0.2)) +
    geom_text(
      aes(label = original_names),
      position = position_jitter(seed = 1, width = 0.2),
      hjust = 0,
      vjust = 0,
      check_overlap = TRUE
    ) +
    ylim(ymin, 1) +
    ylab(paste0(
      "Mean Pearson Correlation Coefficient by ",
      treatment_var,
      " and ",
      dose_var
    )) +
    xlab(paste(treatment_var, dose_var, sep = " x ")) +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 90, hjust = 1))
  
  # Save the plot
  ggsave(
    gg,
    file = paste0(
      paths$output,
      "/Samp_for_samp_corr_by_treatment_dose_comparison_boxplot.pdf"
    )
  )
  
  # Print the plot
  print(gg)
}
```

## Reference RNA

This tests the Pearson correlation of log2-normalized CPM of read counts. If your data includes reference RNA, it should be shown here. If your data includes two sources of reference RNA, the same standard mix should shower higher correlations than between different standards. The correlations for the same standards across different batches/plates should be relatively high.

Note-Jory: Our datasets do not contain reference RNA

```{r correlations_7, fig.width = 11, fig.height = 10, out.height='100%', out.width='100%', eval=(length(ref_samples) > 0)}
#ref_correlations <- cor(as.matrix(cpm %>%
#                                      dplyr::select(all_of(ref_samples))),
#                          method = "pearson")

#correlation_df <- DESeqDesign %>%
#  dplyr::filter(original_names %in% colnames(ref_correlations))

#row.names(correlation_df) <- correlation_df$original_names
#correlation_df <- correlation_df[colnames(ref_correlations),]

#correlation_df <- as.data.frame(correlation_df[,unlist(groups[[1]])], drop = FALSE)

#pheatmap(ref_correlations,
#         annotation_col = correlation_df,
#         display_numbers = T,
#         cutree_cols = 2,
#         cutree_rows = 2)

#heatmap_colors <- circlize::colorRamp2(c(0.7, 1), c("blue", "red"))

#annotation_col <- HeatmapAnnotation(df = correlation_df)

# Heatmap(ref_correlations,
#         name = "Correlation",
#         clustering_distance_rows = "pearson",
#         clustering_distance_columns = "pearson",
#         row_km = 2,
#         column_km = 2,
#         cell_fun = function(j, i, x, y, width, height, fill) {
#           grid.text(sprintf("%.2f", ref_correlations[i, j]), x, y, gp = gpar(fontsize = 10))
#           },
#         top_annotation = annotation_col,
#         col = heatmap_colors
#         )

# ref_correlations

```

# {-}

# Compose tables

```{r return_data}


################################################################################
# Add Dendrogram Clustering Pass/Fail to QAQC
################################################################################

# Initialize all samples as "PASS" for dendrogram clustering
QAQC$dendrogram <- "PASS"

# Check if `flag_cluster` exists and update the dendrogram status
if (exists("flag_cluster")) {
  QAQC$dendrogram[QAQC$Sample %in% d$original_names[flag_cluster]] <- "FAIL"
}

# Identify samples that failed the dendrogram clustering
failed_dendro_clustering <- QAQC[QAQC$dendrogram == "FAIL", ]

# Check and print flagged samples
#print("Samples Flagged in Dendrogram Clustering:")
#if (nrow(failed_dendro_clustering) > 0) {
#  print(failed_dendro_clustering$Sample)
#} else {
#  print("No samples failed the dendrogram clustering.")
#}

################################################################################
# Removing Outliers
################################################################################

# Initialize pass/fail for each metric in QAQC
QAQC_pass_fail <- QAQC

# Apply pass/fail criteria for each metric
QAQC_pass_fail <- QAQC_pass_fail %>%
  dplyr::mutate(
    NMR = cut(NMR, c(-Inf, nmr_threshold, Inf), labels = c("FAIL", "PASS"), right = FALSE),
    Ncov5 = cut(Ncov5, 
                 breaks = c(-Inf, percent_active_probes$stats[1, 1] - 0.001, percent_active_probes$stats[5, 1], Inf), 
                 labels = c("FAIL", "PASS", "FAIL"), 
                 right = TRUE),
    Nsig80 = cut(Nsig80, 
                 breaks = c(-Inf, probes_capturing_top_80$stats[1, 1] - 1, probes_capturing_top_80$stats[5, 1], Inf), 
                 labels = c("FAIL", "PASS", "FAIL"), 
                 right = TRUE),
    Gini = cut(Gini, c(-Inf, gini_cutoff, Inf), labels = c("PASS", "FAIL"), right = FALSE),
    pct_mapped = cut(pct_mapped, c(-Inf, align_threshold * 100, Inf), labels = c("FAIL", "PASS"), right = FALSE),
    q30 = cut(q30, c(-Inf, q30_cutoff * 100, Inf), labels = c("FAIL", "PASS"), right = FALSE),
    diff_r1_r2_q30 = cut(diff_r1_r2_q30, c(-Inf, forward_reverse_q30_diff_cutoff * 100, Inf), labels = c("PASS", "FAIL"), right = FALSE),
    PC1_var = cut(PC1_var, c(-Inf, PCA_cutoff * 100, Inf), labels = c("PASS", "FAIL"), right = FALSE),
    PC2_var = cut(PC2_var, c(-Inf, PCA_cutoff * 100, Inf), labels = c("PASS", "FAIL"), right = FALSE)
  )

# Filter rows where any metric has a "FAIL" value
QAQC_failed <- QAQC_pass_fail %>%
  dplyr::rowwise() %>%
  dplyr::filter(any(c_across(everything()) == "FAIL"))

# Convert "FAIL"/"PASS" to 1/0 for easier processing
QAQC_failed_logical <- QAQC_failed %>%
  dplyr::mutate(
    Gini = if_else(Gini == "FAIL", 1, 0),
    Nsig80 = if_else(Nsig80 == "FAIL", 1, 0),
    Ncov5 = if_else(Ncov5 == "FAIL", 1, 0),
    dendrogram = if_else(dendrogram == "FAIL", 1, 0),
    NMR = if_else(NMR == "FAIL", 1, 0),
    pct_mapped = if_else(pct_mapped == "FAIL", 1, 0),
    q30 = if_else(q30 == "FAIL", 1, 0),
    diff_r1_r2_q30 = if_else(diff_r1_r2_q30 == "FAIL", 1, 0),
    PC1_var = if_else(PC1_var == "FAIL", 1, 0),
    PC2_var = if_else(PC2_var == "FAIL", 1, 0)
  )

# Join with metadata to include treatment and dose information
QAQC_failed_logical <- QAQC_failed_logical %>%
  dplyr::left_join(d %>% dplyr::select(original_names, !!sym(treatment_var), !!sym(dose)),
                   by = c("Sample" = "original_names"))

# Ensure numeric conversion for all fail indicators
QAQC_failed_logical <- QAQC_failed_logical %>%
  dplyr::mutate(
    NMR = as.numeric(NMR),
    Ncov5 = as.numeric(Ncov5),
    Nsig80 = as.numeric(Nsig80),
    Gini = as.numeric(Gini),
    dendrogram = as.numeric(dendrogram),
    pct_mapped = as.numeric(pct_mapped),
    q30 = as.numeric(q30),
    diff_r1_r2_q30 = as.numeric(diff_r1_r2_q30),
    PC1_var = as.numeric(PC1_var),
    PC2_var = as.numeric(PC2_var)
  )

print("QC table composed successfully.")
```

# Outliers {.tabset}

This section shows several tables of the outlier data.  

## UpSet plot

This plot shows the various QC metrics that may result in sample removal, along with the numbers of samples that were removed for one or more of those filters.  

If the "UpSet" plot makes you upset, please skip to the much more intuitive "vtree".

```{r upset_plot, out.height='100%', out.width='100%'}
# Prepare data for UpSet plot
upset_data <- QAQC_failed_logical[, c(
  "NMR",
  "dendrogram",
  "Ncov5",
  "Nsig80",
  "Gini",
  "pct_mapped",
  "q30",
  "diff_r1_r2_q30",
  "PC1_var",
  "PC2_var"
)]

# Convert to logical, then to integer for the UpSet plot
upset_data <- upset_data %>%
  dplyr::mutate(across(everything(), as.logical)) %>%
  dplyr::mutate(across(everything(), as.integer))

# Convert the data frame to a list for the UpSet plot
upset_data_list <- as.list(upset_data)

# Check if there are multiple samples that failed, then generate the UpSet plot
if (nrow(QAQC_failed_logical) > 1) {
  png(
    file = file.path(paths$output, "upset_plot.png"),
    units = "in",
    width = 8,
    height = 8,
    res = 300
  )
  print(upset(fromList(upset_data_list)))
  dev.off()
  include_safe_graphics(file.path(paths$output, "upset_plot.png"))
} else {
  print("No samples failed QC. Skipping upset plot.")
}
```

## Part one - Vtree plot

```{r vtree_plot_1, collapse=TRUE, out.height='100%', out.width='100%'}
# Join DESeqDesign with QAQC_pass_fail and QAQC
QAQC_annotated <- dplyr::left_join(DESeqDesign, QAQC_pass_fail, by = c("original_names" = "Sample")) %>%
  dplyr::right_join(QAQC, by = c("original_names" = "Sample"), suffix = c("", "_data"))

# Create a new column 'Any' to indicate if any metric has failed, ignoring NAs
QAQC_annotated$Any <- apply(
  QAQC_annotated,
  MARGIN = 1,
  FUN = function(x) {
    # Filter out NA values and only consider character/factor values
    valid_values <- x[!is.na(x) & (is.character(x) | is.factor(x))]
    
    # Check if any valid value is "FAIL"
    if (any(valid_values == "FAIL", na.rm = TRUE)) {
      return("FAIL")
    } else {
      return("PASS")
    }
  }
)

# Generate vtree plots
vtree1 <- vtree(
  QAQC_annotated,
  c("dendrogram", "NMR", "Ncov5", "Nsig80", "Gini", "pct_mapped", "q30", "diff_r1_r2_q30", "PC1_var", "PC2_var", "Any"),
  summary = "NMR_data \nAverage Reads Mapped\n%mean% %leafonly% ",
  pngknit = FALSE
)

vtree2 <- vtree(
  QAQC_annotated,
  c("dendrogram", "NMR", "Ncov5", "Nsig80", "Gini", "pct_mapped", "q30", "diff_r1_r2_q30", "PC1_var", "PC2_var"),
  summary = "NMR_data \nAverage Reads Mapped\n%mean% %leafonly% ",
  pattern = TRUE,
  pngknit = FALSE
)

# Save the vtree plots as PNG files
grVizToPNG(vtree1, folder = paths$output, filename = "Vtree_Plot1.png")
grVizToPNG(vtree2, folder = paths$output, filename = "Vtree_Plot2.png")

# Include the vtree plots in the report
include_safe_graphics(file.path(paths$output, "Vtree_Plot1.png"))
```

## Part two - Vtree plot

```{r vtree_plot_2, collapse=TRUE, out.height='100%', out.width='100%'}
include_safe_graphics(file.path(paths$output, "Vtree_Plot2.png"))
```

## Outlier sample names

```{r outlier_sample_names}
# Extract outlier samples from QAQC_failed
outliers <- QAQC_failed$Sample
if (length(outliers) > 0) {print(outliers)} else {print("No outlier samples")}

# Filter metadata for outlier samples
outlier_metadata <- DESeqDesign %>%
  dplyr::filter(original_names %in% outliers)

# Filter metadata for samples with outliers removed
metadata_outliers_removed <- DESeqDesign %>%
  dplyr::filter(!original_names %in% outliers) %>%
  dplyr::filter(!!ensym(technical_control) == FALSE)
```

## Potential covariates

This examines correlations between the QC metrics calculated and any factors in the experimental design.  

Please note that in these pairwise plots, some relationships are meaningless (e.g., dose vs dose). This is meant to explore trends that may not otherwise be apparent.  

```{r possible_covariates, fig.width = 11, fig.height = 10, out.height='100%', out.width='100%'}


# Define common metrics columns that should be included in all plots
metrics_cols <- c(
  "NMR_data", "Ncov5_data", "Nsig80_data", "Gini_data", 
  "pct_mapped_data", "q30_data", "diff_r1_r2_q30_data", 
  "PC1_var", "PC2_var"
)

# Wrap the entire section in tryCatch to ensure script continues
tryCatch({
  # Extract QAQC metadata as you had before
  QAQC_metadata_subset <- QAQC_annotated %>%
    dplyr::select(
      unlist(groups),
      NMR_data,
      Ncov5_data,
      Nsig80_data,
      Gini_data,
      pct_mapped_data,
      q30_data,
      diff_r1_r2_q30_data,
      PC1_var,
      PC2_var
    )

  # Identify potential covariates with fewer than 24 unique levels
  potential_covariates <- QAQC_metadata_subset %>%
    dplyr::mutate_all(as.factor) %>%
    purrr::map(levels) %>%
    purrr::map(length)

  covariate_names <-
    names(potential_covariates[potential_covariates < 24])

}, error = function(e) {
  message("Error in data preparation: ", e$message)
  # Create empty objects to prevent errors in subsequent code
  QAQC_metadata_subset <<- data.frame()
  covariate_names <<- character(0)
})

# Improved function with better error handling
create_ggpairs_plot <- function(data, columns, color_var = NULL, output_file = NULL) {
  # Exit early if data is empty
  if(nrow(data) == 0 || ncol(data) == 0) {
    message("Cannot create plot: Empty dataset")
    return(NULL)
  }
  
  # Wrap everything in tryCatch
  tryCatch({
    # Filter to only include columns that exist in the data
    valid_columns <- columns[columns %in% names(data)]
    
    if(length(valid_columns) == 0) {
      message("No valid columns found for plotting")
      return(NULL)
    }
    
    # Create the plot with or without color
    if (!is.null(color_var) && color_var %in% names(data)) {
      # Direct column reference for color
      p <- ggpairs(
        data,
        cardinality_threshold = 24,
        columns = valid_columns,
        mapping = aes_string(color = color_var)
      )
    } else {
      p <- ggpairs(
        data,
        cardinality_threshold = 24,
        columns = valid_columns
      )
    }
    
    # Print and save if requested
    tryCatch({
      suppressWarnings(print(p))
      if (!is.null(output_file)) {
        suppressWarnings(ggsave(output_file, p, width = 10, height = 8))
        message("Saved plot to: ", output_file)
      }
      return(p)
    }, error = function(e) {
      message("Error printing/saving plot: ", e$message)
      return(NULL)
    })
  }, error = function(e) {
    message("Error creating plot: ", e$message)
    return(NULL)
  })
}

# Make each plotting section fail independently
# 1. Plot for each covariate
tryCatch({
  for (i in seq_along(covariate_names)) {
    # Wrap each iteration in tryCatch
    tryCatch({
      cov_name <- covariate_names[i]
      message("Creating plot for covariate: ", cov_name)
      create_ggpairs_plot(
        QAQC_metadata_subset,
        columns = c(cov_name, metrics_cols),
        color_var = groups[[1]][1]
      )
    }, error = function(e) {
      message("Error in covariate plot #", i, ": ", e$message)
    })
  }
}, error = function(e) {
  message("Error in covariate plotting section: ", e$message)
})

# 2. Generate plots for each group
tryCatch({
  for (i in seq_along(groups)) {
    tryCatch({
      group_cols <- unlist(groups[[i]])
      message("Creating plot for group: ", i)
      create_ggpairs_plot(
        QAQC_metadata_subset,
        columns = c(group_cols, metrics_cols),
        color_var = groups[[1]][1]
      )
    }, error = function(e) {
      message("Error in group plot #", i, ": ", e$message)
    })
  }
}, error = function(e) {
  message("Error in group plotting section: ", e$message)
})

# 3. Handle batch variable if present
tryCatch({
  if (!is.null(batch_var) && batch_var %in% names(QAQC_metadata_subset)) {
    # For each group beyond the first (if multiple groups)
    if (length(groups) > 1) {
      for (i in seq_along(groups)[2:length(groups)]) {
        tryCatch({
          message("Creating batch plot for group: ", i)
          create_ggpairs_plot(
            QAQC_metadata_subset,
            columns = c(unlist(groups[[i]]), metrics_cols),
            color_var = batch_var,
            output_file = paste0(paths$output, "/Batch_group_", i, "_plot.pdf")
          )
        }, error = function(e) {
          message("Error in batch group plot #", i, ": ", e$message)
        })
      }
    }
    
    # Create plot with all groups colored by batch
    tryCatch({
      message("Creating final plot with batch coloring")
      create_ggpairs_plot(
        QAQC_metadata_subset,
        columns = c(unlist(groups), metrics_cols),
        color_var = batch_var,
        output_file = paste0(paths$output, "/Potential_covariates_plot.pdf")
      )
    }, error = function(e) {
      message("Error in final batch plot: ", e$message)
    })
  } else {
    # Final plot without batch coloring
    tryCatch({
      message("Creating final plot without batch coloring")
      create_ggpairs_plot(
        QAQC_metadata_subset,
        columns = c(unlist(groups), metrics_cols),
        output_file = paste0(paths$output, "/Potential_covariates_plot.pdf")
      )
    }, error = function(e) {
      message("Error in final plot: ", e$message)
    })
  }
}, error = function(e) {
  message("Error in batch variable section: ", e$message)
})

message("Plotting section completed - script will continue")

```

## Plate position effects

```{r plate_pos, fig.width = 11, fig.height = 10, out.height='100%', out.width='100%', eval = any(grepl("column", colnames(QAQC_annotated), ignore.case = T))}
#Evaluate if you have plate positions in metadata

ggplatemappedreads <- ggplot(QAQC_annotated, aes(x = factor(column),
                                                 y = factor(row, levels = rev(levels(
                                                   factor(row)
                                                 ))))) +
  geom_tile(mapping = aes(fill = log(Dose + 1)), data = QAQC_annotated) +
  scale_fill_gradient(low = "grey", high = "black") +
  geom_point(mapping = aes(color = NMR_data),
             data = QAQC_annotated,
             size = 3) +
  scale_x_discrete(position = "top") +
  coord_fixed() +
  #There are no batch vars for the downloaded datasets... OR are there!?
  #facet_wrap(paste0("~",batch_var)) +
  ggtitle("Number of mapped reads") +
  theme_bw() +
  theme(
    legend.position = "bottom",
    axis.title.x = element_blank(),
    axis.title.y = element_blank()
  )

ggplatensig80 <- ggplot(QAQC_annotated, aes(x = factor(column),
                                            y = factor(row, levels = rev(levels(
                                              factor(row)
                                            ))))) +
  geom_tile(mapping = aes(fill = log(Dose + 1)), data = QAQC_annotated) +
  scale_fill_gradient(low = "grey", high = "black") +
  geom_point(
    mapping = aes(color = Nsig80_data),
    data = QAQC_annotated,
    size = 3
  ) +
  scale_x_discrete(position = "top") +
  coord_fixed() +
  #There are no batch vars for the downloaded datasets... OR are there!?
  #facet_wrap(paste0("~",batch_var)) +
  ggtitle("Nsig80") +
  theme_bw() +
  theme(
    legend.position = "bottom",
    axis.title.x = element_blank(),
    axis.title.y = element_blank()
  )

ggplatencov5 <- ggplot(QAQC_annotated, aes(x = factor(column),
                                           y = factor(row, levels = rev(levels(
                                             factor(row)
                                           ))))) +
  geom_tile(mapping = aes(fill = log(Dose + 1)), data = QAQC_annotated) +
  scale_fill_gradient(low = "grey", high = "black") +
  geom_point(mapping = aes(color = Ncov5_data),
             data = QAQC_annotated,
             size = 3) +
  scale_x_discrete(position = "top") +
  coord_fixed() +
  #There are no batch vars for the downloaded datasets... OR are there!?
  #facet_wrap(paste0("~",batch_var)) +
  ggtitle("Ncov5") +
  theme_bw() +
  theme(
    legend.position = "bottom",
    axis.title.x = element_blank(),
    axis.title.y = element_blank()
  )

ggplategini <- ggplot(QAQC_annotated, aes(x = factor(column),
                                          y = factor(row, levels = rev(levels(
                                            factor(row)
                                          ))))) +
  geom_tile(mapping = aes(fill = log(Dose + 1)), data = QAQC_annotated) +
  scale_fill_gradient(low = "grey", high = "black") +
  geom_point(mapping = aes(color = Gini_data),
             data = QAQC_annotated,
             size = 3) +
  scale_x_discrete(position = "top") +
  coord_fixed() +
  #There are no batch vars for the downloaded datasets... OR are there!?
  #facet_wrap(paste0("~",batch_var)) +
  ggtitle("Gini") +
  theme_bw() +
  theme(
    legend.position = "bottom",
    axis.title.x = element_blank(),
    axis.title.y = element_blank()
  )

ggplatefailed <- ggplot(QAQC_annotated, aes(x = factor(column),
                                            y = factor(row, levels = rev(levels(
                                              factor(row)
                                            ))))) +
  geom_tile(mapping = aes(fill = log(Dose + 1)), data = QAQC_annotated) +
  scale_fill_gradient(low = "grey", high = "black") +
  geom_point(mapping = aes(color = Any),
             data = QAQC_annotated,
             size = 3) +
  scale_x_discrete(position = "top") +
  coord_fixed() +
  #There are no batch vars for the downloaded datasets... OR are there!?
  #facet_wrap(paste0("~",batch_var)) +
  ggtitle("Any samples failed") +
  theme_bw() +
  theme(
    legend.position = "bottom",
    axis.title.x = element_blank(),
    axis.title.y = element_blank()
  )

ggsave(ggplatemappedreads,
       file = paste0(paths$output, "/PlatePosition_Mapped_reads_plot.pdf"))
ggsave(ggplatensig80,
       file = paste0(paths$output, "/PlatePosition_NSig80_plot.pdf"))
ggsave(ggplatencov5,
       file = paste0(paths$output, "/PlatePosition_NCov5_plot.pdf"))
ggsave(ggplategini,
       file = paste0(paths$output, "/PlatePosition_Gini_plot.pdf"))
ggsave(
  ggpplatefailed,
  file = paste0(
    paths$output,
    "/PlatePosition_Samples_failing_filters_plot.pdf"
  )
)

print(ggplatemappedreads)
print(ggplatensig80)
print(ggplatencov5)
print(ggplategini)
print(ggpplatefailed)
#include_safe_graphics(paste0(paths$output, "/PlatePosition_Mapped_reads_plot.pdf"))
#include_safe_graphics(paste0(paths$output, "/PlatePosition_NSig80_plot.pdf"))
#include_safe_graphics(paste0(paths$output, "/PlatePosition_NCov5_plot.pdf"))
#include_safe_graphics(paste0(paths$output, "/PlatePosition_Gini_plot.pdf"))
#include_safe_graphics(paste0(paths$output, "/PlatePosition_Samples_failing_filters_plot.pdf"))
```


## Metadata of samples identified as outliers

```{r table_metadata}


knitr::kable(outlier_metadata,
             caption = "Samples Removed") %>%
  kable_styling(
    bootstrap_options = "striped",
    full_width = F,
    position = "left"
  ) %>%
  kableExtra::scroll_box(width = "100%", height = "480px")

QAQC_failed_kable <- outlier_metadata %>%
  dplyr::select(original_names,!!ensym(treatment_var)) %>%
  dplyr::left_join(QAQC_failed, by = (c("original_names" = "Sample")))

```

## Pass/fail table for samples identified as outliers

```{r table_pass_fail}


knitr::kable(QAQC_failed_kable,
             caption = "Which samples were removed and why?") %>%
  kable_styling(
    bootstrap_options = "striped",
    full_width = F,
    position = "left"
  ) %>%
  scroll_box(width = "100%", height = "480px")


```

## Summary of pass/fail for each test

```{r pass_fail_summary}
#QAQC_annotated %>%
#  dplyr::group_by(dendrogram, NMR, Ncov5, Nsig80, Gini, pct_mapped, q30, diff_r1_r2_q30, PC1_var, PC2_var, Any) %>%
#  tally()

QAQC_annotated %>%
  dplyr::group_by(
    dendrogram,
    NMR,
    Ncov5,
    Nsig80,
    Gini,
    pct_mapped,
    q30,
    diff_r1_r2_q30,
    PC1_var,
    PC2_var,
    Any
  ) %>%
  dplyr::tally() %>%
  knitr::kable(caption = "Summary of pass/fail results") %>%
  kable_styling(
    bootstrap_options = "striped",
    full_width = F,
    position = "left"
  ) %>%
  scroll_box(width = "100%", height = "480px")
```

## Pass/fail for each test, broken up by sample groups

```{r pass_fail_by_group}
#QAQC_annotated %>%
#  dplyr::group_by(dendrogram, NMR, Ncov5, Nsig80, Gini, pct_mapped, q30, diff_r1_r2_q30, PC1_var, PC2_var, Any, across(all_of(unlist(groups)))) %>%
#  tally()

QAQC_annotated %>%
  dplyr::group_by(
    dendrogram,
    NMR,
    Ncov5,
    Nsig80,
    Gini,
    pct_mapped,
    q30,
    diff_r1_r2_q30,
    PC1_var,
    PC2_var,
    Any,
    across(all_of(unlist(groups)))
  ) %>%
  dplyr::tally() %>%
  knitr::kable(caption = "Pass/fail results by group") %>%
  kable_styling(
    bootstrap_options = "striped",
    full_width = F,
    position = "left"
  ) %>%
  scroll_box(width = "100%", height = "480px")
```

## Write tables to disk

```{r write_output}


outlier_metadata_annotated <- dplyr::left_join(outlier_metadata,
                                               QAQC_pass_fail,
                                               by = c("original_names" = "Sample"))
write.table(
  outlier_metadata_annotated,
  normalizePath(file.path(paths$output, "samples_removed.txt")),
  sep = "\t",
  row.names = FALSE,
  col.names = TRUE,
  quote = FALSE
)

write.table(
  QAQC_annotated,
  normalizePath(file.path(paths$output, "QC_per_sample.txt")),
  sep = "\t",
  row.names = FALSE,
  col.names = TRUE,
  quote = FALSE
)

write.table(
  metadata_outliers_removed,
  normalizePath(file.path(paths$output, "metadata.QC_applied.txt")),
  sep = "\t",
  row.names = FALSE,
  col.names = TRUE,
  quote = FALSE
)

sampleData_filtered <-
  sampleData %>% dplyr::select(-all_of(outliers))
print(paste(
  "Writing sample data for",
  ncol(sampleData_filtered),
  "filtered samples"
))
```

# Post-QC and data exploration {.tabset}

## Dendrogram, all samples, postfiltering

```{r dendrogram_all_after, echo = FALSE, fig.width = 11, fig.height = 10, out.height='100%', out.width='100%'}
############################################################################
# Dendrogram: all, after filtering
############################################################################

CairoPDF(
  file = normalizePath(file.path(
    paths$output, "dendrogram_postfiltering_all.pdf"
  )),
  width = 14,
  height = 8.5,
  pointsize = 12,
  family = "Ubuntu"
)

dendro_after_all_samples <-
  1 - cor(as.matrix(cpm %>% dplyr::select(-all_of(outliers))),
          method = "spearman")

if (dim(dendro_after_all_samples)[1] < 2) {
  print("Not enough samples left after filtering to create a dendrogram")
} else {
  dendro_after_all_samples <-
  sort_hclust(hclust(as.dist(dendro_after_all_samples), method = "average"))

  dendro_after_all_samples <- as.dendrogram(dendro_after_all_samples)
  colors_to_use <-
    as.numeric(as.factor(DESeqDesign[, technical_control]))
  ordered_colors <-
    colors_to_use[order.dendrogram(dendro_after_all_samples)]
  labels_colors(dendro_after_all_samples) <- ordered_colors

  original_names <- DESeqDesign[, "original_names"]
  original_names <-
    original_names[order.dendrogram(dendro_after_all_samples)]

  labels_to_use <- DESeqDesign[, dendro_color_by]
  labels_to_use <-
    labels_to_use[order.dendrogram(dendro_after_all_samples)]
  labels_to_use <- paste(labels_to_use, original_names)

  #par(cex = 0.15, mar = c(15, 4, 4, 2))
  plot(
    dendro_after_all_samples %>% dendextend::set("labels", labels_to_use),
    main = "log2 CPM",
    horiz = F
  )
  abline(h = tree_height_cutoff, col = "red", lwd = 2)
  dev.off()
}
```

## Dendrogram, technical control samples, postfiltering

```{r dendrogram_technical_after, echo = FALSE, fig.width = 11, fig.height = 10, out.height='100%', out.width='100%'}
############################################################################
# Technical Controls, Postfiltering
############################################################################
dendro_after_tech_ctrls <- 1 - cor(as.matrix(cpm %>%
                                               dplyr::select(all_of(tech_ctrl_names)) %>%
                                               dplyr::select(-all_of(outliers[outliers %in% tech_ctrl_names]))),
                                   method = "spearman")
if (dim(dendro_after_tech_ctrls)[1] > 2) {
  DESeqDesignTechCtrls <-
    DESeqDesign %>% dplyr::filter(original_names %in% all_of(tech_ctrl_names))
  
  dendro_after_tech_ctrls <-
    sort_hclust(hclust(as.dist(dendro_after_tech_ctrls), method = "average"))
  
  dendro_after_tech_ctrls <- as.dendrogram(dendro_after_tech_ctrls)
  colors_to_use <-
    as.numeric(as.factor(DESeqDesignTechCtrls[, dendro_color_by]))
  ordered_colors <-
    colors_to_use[order.dendrogram(dendro_after_tech_ctrls)]
  labels_colors(dendro_after_tech_ctrls) <- ordered_colors
  
  original_names <- DESeqDesignTechCtrls[, "original_names"]
  original_names <-
    original_names[order.dendrogram(dendro_after_tech_ctrls)]
  
  labels_to_use <- DESeqDesignTechCtrls[, dendro_color_by]
  labels_to_use <-
    labels_to_use[order.dendrogram(dendro_after_tech_ctrls)]
  labels_to_use <- paste(labels_to_use, original_names)
  
  CairoPDF(
    file = normalizePath(
      file.path(
        paths$output,
        "dendrogram_postfiltering_tech_controls.pdf"
      )
    ),
    width = 14,
    height = 8.5,
    family = "Ubuntu"
  )
  #par(cex = 0.4, mar = c(15, 4, 4, 2))
  plot(
    dendro_after_tech_ctrls %>% dendextend::set("labels", labels_to_use),
    main = "log2 CPM",
    horiz = F
  )
  abline(h = tree_height_cutoff, col = "red", lwd = 2)
  dev.off()

  include_safe_graphics(file.path(paths$output, "dendrogram_postfiltering_tech_controls.pdf"))
} else {
  print("Not enough samples left after filtering to create the dendrogram")
}


```

## Dendrogram, experimental samples, postfiltering

```{r dendrogram_exp_after, echo = FALSE, fig.width = 11, fig.height = 10, out.height='100%', out.width='100%'}
############################################################################
# Experimental Samples, Postfiltering
############################################################################

CairoPDF(
  file = normalizePath(
    file.path(paths$output,
              "dendrogram_postfiltering_exp_samples.pdf")
  ),
  width = 14,
  height = 8.5,
  family = "Ubuntu"
)

dendro_after_all_exp_samples <- 1 - cor(as.matrix(cpm %>%
                                                    dplyr::select(all_of(sample_names)) %>%
                                                    dplyr::select(-all_of(outliers[outliers %in% sample_names]))),
                                        method = "spearman")

if (dim(dendro_after_all_exp_samples)[1] < 2) {
  print("Not enough samples left after filtering to create a dendrogram")
} else {
  DESeqDesignExpSamples <-
    DESeqDesign %>% dplyr::filter(original_names %in% all_of(sample_names))

  dendro_after_all_exp_samples <-
    sort_hclust(hclust(as.dist(dendro_after_all_exp_samples), method = "average"))

  dendro_after_all_exp_samples <-
    as.dendrogram(dendro_after_all_exp_samples)
  colors_to_use <-
    as.numeric(as.factor(DESeqDesignExpSamples[, unlist(groups)[1]]))
  ordered_colors <-
    colors_to_use[order.dendrogram(dendro_after_all_exp_samples)]
  labels_colors(dendro_after_all_exp_samples) <- ordered_colors

  original_names <- DESeqDesignExpSamples[, "original_names"]
  original_names <-
    original_names[order.dendrogram(dendro_after_all_exp_samples)]

  labels_to_use <- DESeqDesignExpSamples[, dendro_color_by]
  labels_to_use <-
    labels_to_use[order.dendrogram(dendro_after_all_exp_samples)]
  labels_to_use <- paste(labels_to_use, original_names)

  #par(cex = 0.2, mar = c(15, 4, 4, 2))
  plot(
    dendro_after_all_exp_samples %>% dendextend::set("labels", labels_to_use),
    main = "log2 CPM",
    horiz = F
  )
  abline(h = tree_height_cutoff, col = "red", lwd = 2)
  dev.off()
}
```

## Dendrogram, experimental samples, postfiltering, colored by dose

```{r dendrogram_exp_post_dose, echo = FALSE, fig.width = 11, fig.height = 10, out.height='100%', out.width='100%'}
############################################################################
# Experimental Samples, Postfiltering, Colored by Dose
############################################################################

CairoPDF(
  file = normalizePath(
    file.path(
      paths$output,
      "dendrogram_postfiltering_exp_samples_by_dose.pdf"
    )
  ),
  width = 14,
  height = 8.5,
  family = "Ubuntu"
)

# Calculate the dendrogram
dendro_after_all_exp_samples <- 1 - cor(as.matrix(cpm %>%
                                                    dplyr::select(all_of(sample_names)) %>%
                                                    dplyr::select(-all_of(outliers[outliers %in% sample_names]))),
                                        method = "spearman")

if (dim(dendro_after_all_exp_samples)[1] < 2) {
  print("Not enough samples left after filtering to create a dendgrogram")
} else {
  DESeqDesignExpSamples <-
  DESeqDesign %>% dplyr::filter(original_names %in% all_of(sample_names))

  dendro_after_all_exp_samples <-
    sort_hclust(hclust(as.dist(dendro_after_all_exp_samples), method = "average"))

  dendro_after_all_exp_samples <-
    as.dendrogram(dendro_after_all_exp_samples)
  colors_to_use <-
    as.numeric(as.factor(DESeqDesignExpSamples[, dendro_color_by]))
  ordered_colors <-
    colors_to_use[order.dendrogram(dendro_after_all_exp_samples)]
  labels_colors(dendro_after_all_exp_samples) <- ordered_colors

  original_names <- DESeqDesignExpSamples[, "original_names"]
  original_names <-
    original_names[order.dendrogram(dendro_after_all_exp_samples)]

  labels_to_use <- DESeqDesignExpSamples[, dendro_color_by]
  labels_to_use <-
    labels_to_use[order.dendrogram(dendro_after_all_exp_samples)]
  labels_to_use <- paste(labels_to_use, original_names)

  # Plot the dendrogram
  plot(
    dendro_after_all_exp_samples %>% dendextend::set("labels", labels_to_use),
    main = "log2 CPM",
    horiz = F
  )
  abline(h = tree_height_cutoff, col = "red", lwd = 2)

  # Create the legend
  unique_labels <- unique(DESeqDesignExpSamples[, dendro_color_by])
  unique_colors <- unique(colors_to_use)

  legend("topright",
         legend = unique_labels,
         fill = unique_colors,
         title = dendro_color_by)

  dev.off()
}
```

# {-}

## Potential batch effects

Only applicable if a batch variable is present in metadata.

```{r plot_batch_effects, fig.width = 11, fig.height = 10, out.height='100%', out.width='100%', eval = !is.null(batch_var)}
ggbatchgini <-
  ggplot(QAQC_annotated, aes(x = batch_var, color = treatment_var)) +
  geom_boxplot(aes(y = Gini_data)) +
  facet_grid(paste0("~", batch_var), scales = "free_x") +
  ggtitle("Gini") +
  theme(legend.position = "bottom")

ggbatchNMR <-
  ggplot(QAQC_annotated, aes(x = batch_var, color = treatment_var)) +
  geom_boxplot(aes(y = NMR_data)) +
  facet_grid(paste0("~", batch_var), scales = "free_x") +
  ggtitle("Number of Mapped Reads") +
  theme(legend.position = "bottom")

ggbatchNCov5 <-
  ggplot(QAQC_annotated, aes(x = batch_var, color = treatment_var)) +
  geom_boxplot(aes(y = Ncov5_data)) +
  facet_grid(paste0("~", batch_var), scales = "free_x") +
  ggtitle("Proportion of active probes (% probes with >5 mapped reads)") +
  theme(legend.position = "bottom")

ggbatchNSig80 <-
  ggplot(QAQC_annotated, aes(x = batch_var, color = treatment_var)) +
  geom_boxplot(aes(y = Nsig80_data)) +
  facet_grid(paste0("~", batch_var), scales = "free_x") +
  ggtitle("Proportion of probes required to account for 80% of signal") +
  theme(legend.position = "bottom")

ggsave(ggbatchgini,
       file = paste0(paths$output, "/Batch_Effects_Gini_plot.pdf"))
ggsave(ggbatchNMR,
       file = paste0(paths$output, "/Batch_Effects_Mapped_reads_plot.pdf"))
ggsave(ggbatchNCov5,
       file = paste0(paths$output, "/Batch_Effects_NCov5_plot.pdf"))
ggsave(ggbatchNSig80,
       file = paste0(paths$output, "/Batch_Effects_NSig80_plot.pdf"))


print(ggbatchNMR)
print(ggbatchNCov5)
print(ggbatchNSig80)
print(ggbatchgini)
```

## Sample filters by dose {.tabset}

```{r plot_chemical_differences, fig.width = 11, fig.height = 10, out.height='100%', out.width='100%', eval=any(grepl("dose", colnames(QAQC_annotated), ignore.case = T))}
#dendrogram, pct_mapped, q30, diff_r1_r2_q30, PC1_var, PC2_var
ggdosegini <-
  ggplot(QAQC_annotated, aes(x = !!ensym(treatment_var), y = Gini_data)) +
  geom_boxplot(outlier.shape = NA, position = position_dodge(width = 0.75)) +  # Remove outlier points from the boxplot itself
  geom_jitter(
    aes(fill = factor(!!ensym(dose))),
    shape = 21,
    position = position_jitterdodge(
      jitter.width = 0.2,
      dodge.width = 0.75,
      seed = 1
    )
  ) +  # Add jittered points with fill color by dose, clustered by dose group
  geom_hline(yintercept = gini_cutoff,
             linetype = "dotted",
             color = "red") +  # Add the Gini threshold line at 0.95
  geom_text(
    data = QAQC_annotated %>% dplyr::filter(Gini == "FAIL"),
    # Filter for samples marked as FAIL
    aes(label = original_names, fill = factor(!!ensym(dose))),
    position = position_jitterdodge(
      jitter.width = 0.2,
      dodge.width = 0.75,
      seed = 1
    ),
    hjust = -0.2,
    vjust = 0,
    check_overlap = TRUE,
    size = 3,
    color = "red"
  ) +  # Add sample names next to outlier points
  ylim(min(QAQC_annotated$Gini_data, na.rm = TRUE), 1) +  # Set dynamic lower limit and upper limit to 1
  ylab("Gini Coefficient") +  # Label for y-axis
  xlab(as.character(treatment_var)) +  # Label for x-axis using the treatment variable name
  theme_minimal() +  # Apply minimal theme
  ggtitle("Gini") +
  scale_fill_discrete(name = as.character(dose))  # Adjust legend title for fill

ggdosenmr <-
  ggplot(dplyr::group_by(QAQC_annotated,!!ensym(dose)),
         aes(x = !!ensym(treatment_var), y = NMR_data)) +
  geom_boxplot(outlier.shape = NA, position = position_dodge(width = 0.75)) +  # Remove outlier points and dodge by dose group
  geom_jitter(
    aes(fill = factor(!!ensym(dose))),
    shape = 21,
    position = position_jitterdodge(
      jitter.width = 0.2,
      dodge.width = 0.75,
      seed = 1
    )
  ) +  # Jittered points with fill color by dose, clustered by dose group
  geom_hline(yintercept = nmr_threshold,
             linetype = "dotted",
             color = "red") +  # Add the threshold line for NMR data
  geom_text(
    data = QAQC_annotated %>% dplyr::filter(NMR == "FAIL"),
    # Filter for samples below the threshold
    aes(label = original_names, fill = factor(!!ensym(dose))),
    position = position_jitterdodge(
      jitter.width = 0.2,
      dodge.width = 0.75,
      seed = 1
    ),
    hjust = -0.2,
    vjust = 0,
    check_overlap = TRUE,
    color = "red",
    size = 3
  ) +  # Add sample names next to points below the threshold
  ylim(0, max(QAQC_annotated$NMR_data, na.rm = TRUE)) +  # Set the y-axis with a lower limit of 0 and dynamic upper limit
  ylab("Number of Mapped Reads") +  # Label for y-axis
  xlab(as.character(treatment_var)) +  # Label for x-axis using the treatment variable name
  theme_minimal() +  # Apply minimal theme
  ggtitle("Total Mapped Reads") +
  scale_fill_discrete(name = as.character(dose))  # Adjust legend title for fill

ggdosencov5 <-
  ggplot(dplyr::group_by(QAQC_annotated,!!ensym(dose)),
         aes(x = !!ensym(treatment_var), y = Ncov5_data)) +
  geom_boxplot(outlier.shape = NA, position = position_dodge(width = 0.75), coef = 3) +  # Remove outlier points and dodge by dose group, and make whiskers extend to 3XIQR
  geom_jitter(
    aes(fill = factor(!!ensym(dose))),
    shape = 21,
    position = position_jitterdodge(
      jitter.width = 0.2,
      dodge.width = 0.75,
      seed = 1
    )
  ) +  # Jittered points with fill color by dose, clustered by dose group
  geom_text(
    data = QAQC_annotated %>% dplyr::filter(Ncov5 == "FAIL"),
    # Filter for samples marked as FAIL
    aes(label = original_names, fill = factor(!!ensym(dose))),
    position = position_jitterdodge(
      jitter.width = 0.2,
      dodge.width = 0.75,
      seed = 1
    ),
    hjust = -0.2,
    vjust = 0,
    check_overlap = TRUE,
    color = "red",
    size = 3
  ) +  # Add sample names next to points marked as FAIL
  ylim(
    min(QAQC_annotated$Ncov5_data, na.rm = TRUE) - 0.01,
    max(QAQC_annotated$Ncov5_data, na.rm = TRUE) + 0.01
  ) +  # Set the y-axis with a dynamic lower limit and upper limit
  ylab("Proportion of Active Probes (% probes with >5 mapped reads)") +  # Label for y-axis
  xlab(as.character(treatment_var)) +  # Label for x-axis using the treatment variable name
  theme_minimal() +  # Apply minimal theme
  ggtitle("NCov5") +
  scale_fill_discrete(name = as.character(dose))  # Adjust legend title for fill

ggdosensig80 <-
  ggplot(dplyr::group_by(QAQC_annotated,!!ensym(dose)),
         aes(x = !!ensym(treatment_var), y = Nsig80_data)) +
  geom_boxplot(outlier.shape = NA, position = position_dodge(width = 0.75), coef = 3) +  # Remove outlier points and dodge by dose group
  geom_jitter(
    aes(fill = factor(!!ensym(dose))),
    shape = 21,
    position = position_jitterdodge(
      jitter.width = 0.2,
      dodge.width = 0.75,
      seed = 1
    )
  ) +  # Jittered points with fill color by dose, clustered by dose group
  geom_text(
    data = QAQC_annotated %>% dplyr::filter(Nsig80 == "FAIL"),
    # Filter for samples marked as FAIL
    aes(label = original_names, fill = factor(!!ensym(dose))),
    position = position_jitterdodge(
      jitter.width = 0.2,
      dodge.width = 0.75,
      seed = 1
    ),
    hjust = -0.2,
    vjust = 0,
    check_overlap = TRUE,
    color = "red",
    size = 3
  ) +  # Add sample names next to points marked as FAIL
  ylim(
    min(QAQC_annotated$Nsig80_data, na.rm = TRUE) - 10,
    max(QAQC_annotated$Nsig80_data, na.rm = TRUE) + 10
  ) +  # Set the y-axis with a dynamic lower limit and upper limit
  ylab("Probes Required to Account for 80% of Signal") +  # Label for y-axis
  xlab(as.character(treatment_var)) +  # Label for x-axis using the treatment variable name
  theme_minimal() +  # Apply minimal theme
  ggtitle("NSig80") +
  scale_fill_discrete(name = as.character(dose))  # Adjust legend title for fill

# Q30 Plot
ggdoseq30 <-
  ggplot(dplyr::group_by(QAQC_annotated,!!ensym(dose)),
         aes(x = !!ensym(treatment_var), y = q30_data)) +
  geom_boxplot(outlier.shape = NA, position = position_dodge(width = 0.75)) +  # Dodge boxplots by dose group
  geom_jitter(
    aes(fill = factor(!!ensym(dose))),
    shape = 21,
    position = position_jitterdodge(
      jitter.width = 0.2,
      dodge.width = 0.75,
      seed = 1
    )
  ) +  # Jitter points by dose
  geom_hline(yintercept = q30_cutoff * 100,
             linetype = "dotted",
             color = "red") +  # Add threshold line
  geom_text(
    data = QAQC_annotated %>% dplyr::filter(q30 == "FAIL"),
    # Filter for samples below the Q30 threshold
    aes(label = original_names, fill = factor(!!ensym(dose))),
    position = position_jitterdodge(
      jitter.width = 0.2,
      dodge.width = 0.75,
      seed = 1
    ),
    hjust = -0.2,
    vjust = 0,
    check_overlap = TRUE,
    color = "red",
    size = 3
  ) +  # Add sample names next to points below the threshold
  ylim(if_else(condition = min(QAQC_annotated$q30_data, na.rm = TRUE) < (q30_cutoff * 100), true = min(QAQC_annotated$q30_data, na.rm = TRUE) - 1, false = (q30_cutoff * 100) - 1), 100) +  # Dynamic y-axis from 0 to max value
  ylab("Q30 Percentage") +  # Y-axis label
  xlab(as.character(treatment_var)) +  # X-axis label
  theme_minimal() +  # Minimal theme
  ggtitle("Q30 percentage") +
  scale_fill_discrete(name = as.character(dose))  # Legend title

# Difference in Q30 Between R1 and R2 Plot
ggdoseq30diff <-
  ggplot(dplyr::group_by(QAQC_annotated,!!ensym(dose)),
         aes(x = !!ensym(treatment_var), y = diff_r1_r2_q30_data)) +
  geom_boxplot(outlier.shape = NA, position = position_dodge(width = 0.75)) +  # Dodge boxplots by dose group
  geom_jitter(
    aes(fill = factor(!!ensym(dose))),
    shape = 21,
    position = position_jitterdodge(
      jitter.width = 0.2,
      dodge.width = 0.75,
      seed = 1
    )
  ) +  # Jitter points by dose
  geom_hline(yintercept = forward_reverse_q30_diff_cutoff * 100,
             linetype = "dotted",
             color = "red") +  # Add threshold line
  geom_text(
    data = QAQC_annotated %>% dplyr::filter(diff_r1_r2_q30 == "FAIL"),
    # Filter for samples below the Q30 difference threshold
    aes(label = original_names, fill = factor(!!ensym(dose))),
    position = position_jitterdodge(
      jitter.width = 0.2,
      dodge.width = 0.75,
      seed = 1
    ),
    hjust = -0.2,
    vjust = 0,
    check_overlap = TRUE,
    color = "red",
    size = 3
  ) +  # Add sample names next to points below the threshold
  ylim(
    0,
    if_else(
      condition = max(QAQC_annotated$diff_r1_r2_q30_data, na.rm = TRUE) > forward_reverse_q30_diff_cutoff * 100,
      true = max(QAQC_annotated$diff_r1_r2_q30_data, na.rm = TRUE) + 1,
      false = (forward_reverse_q30_diff_cutoff * 100) + 1
    )
  ) +  # Dynamic y-axis from 0 to max value
  ylab("Difference in Q30 Between R1 and R2") +  # Y-axis label
  xlab(as.character(treatment_var)) +  # X-axis label
  theme_minimal() +  # Minimal theme
  ggtitle("Difference in Q30 Between R1 and R2") +
  scale_fill_discrete(name = as.character(dose))  # Legend title

# Percentage of Mapped Reads Plot
ggdosepctmapped <-
  ggplot(dplyr::group_by(QAQC_annotated,!!ensym(dose)),
         aes(x = !!ensym(treatment_var), y = pct_mapped_data)) +
  geom_boxplot(outlier.shape = NA, position = position_dodge(width = 0.75)) +  # Dodge boxplots by dose group
  geom_jitter(
    aes(fill = factor(!!ensym(dose))),
    shape = 21,
    position = position_jitterdodge(
      jitter.width = 0.2,
      dodge.width = 0.75,
      seed = 1
    )
  ) +  # Jitter points by dose
  geom_hline(yintercept = align_threshold * 100,
             linetype = "dotted",
             color = "red") +  # Add threshold line at 70%
  geom_text(
    data = QAQC_annotated %>% dplyr::filter(pct_mapped == "FAIL"),
    # Filter for samples below the percentage mapped threshold
    aes(label = original_names, fill = factor(!!ensym(dose))),
    position = position_jitterdodge(
      jitter.width = 0.2,
      dodge.width = 0.75,
      seed = 1
    ),
    hjust = -0.2,
    vjust = 0,
    check_overlap = TRUE,
    color = "red",
    size = 3
  ) +  # Add sample names next to points below the threshold
  ylim(if_else(
    condition = min(QAQC_annotated$pct_mapped_data, na.rm = TRUE) < align_threshold * 100,
    true = min(QAQC_annotated$pct_mapped_data, na.rm = TRUE),
    false = (align_threshold * 100) - 1
  ),
  100) +  # Dynamic y-axis from 0 to max value
  ylab("Percentage of Mapped Reads") +  # Y-axis label
  xlab(as.character(treatment_var)) +  # X-axis label
  theme_minimal() +  # Minimal theme
  ggtitle("% Mapped") +
  scale_fill_discrete(name = as.character(dose))  # Legend title

# PC1 Variance Plot
ggdosepc1var <-
  ggplot(dplyr::group_by(QAQC_annotated,!!ensym(dose)),
         aes(x = !!ensym(treatment_var), y = PC1_var_data)) +
  geom_boxplot(outlier.shape = NA, position = position_dodge(width = 0.75)) +  # Dodge boxplots by dose group
  geom_jitter(
    aes(fill = factor(!!ensym(dose))),
    shape = 21,
    position = position_jitterdodge(
      jitter.width = 0.2,
      dodge.width = 0.75,
      seed = 1
    )
  ) +  # Jitter points by dose
  geom_hline(yintercept = PCA_cutoff * 100,
             linetype = "dotted",
             color = "red") +  # Add threshold line at 20%
  geom_text(
    data = QAQC_annotated %>% dplyr::filter(PC1_var == "FAIL"),
    # Filter for samples below the PC1 variance threshold
    aes(label = original_names, fill = factor(!!ensym(dose))),
    position = position_jitterdodge(
      jitter.width = 0.2,
      dodge.width = 0.75,
      seed = 1
    ),
    hjust = -0.2,
    vjust = 0,
    check_overlap = TRUE,
    color = "red",
    size = 3
  ) +  # Add sample names next to points below the threshold
  ylim(0, max(QAQC_annotated$PC1_var_data, na.rm = TRUE)) +  # Dynamic y-axis from 0 to max value
  ylab("PC1 Variance") +  # Y-axis label
  xlab(as.character(treatment_var)) +  # X-axis label
  theme_minimal() +  # Minimal theme
  ggtitle("Variance of PC1 Across Samples\nRelative to Median in Dose Groups by Treatment") +
  scale_fill_discrete(name = as.character(dose))  # Legend title

ggsave(ggdosegini,
       file = paste0(paths$output, "/Dose_Effects_Gini_plot.pdf"))
ggsave(ggdosenmr,
       file = paste0(paths$output, "/Dose_Effects_Mapped_reads_plot.pdf"))
ggsave(ggdosencov5,
       file = paste0(paths$output, "/Dose_Effects_NCov5_plot.pdf"))
ggsave(ggdosensig80,
       file = paste0(paths$output, "/Dose_Effects_NSig80_plot.pdf"))
ggsave(ggdoseq30, file = paste0(paths$output, "/Dose_Effects_Q30_plot.pdf"))
ggsave(ggdoseq30diff,
       file = paste0(paths$output, "/Dose_Effects_DiffQ30_plot.pdf"))
ggsave(ggdosepctmapped,
       file = paste0(paths$output, "/Dose_Effects_Pct_Mapped_plot.pdf"))
ggsave(ggdosepc1var,
       file = paste0(paths$output, "/Dose_Effects_PC1_plot.pdf"))
```

### Q30 Scores

```{r plot_chemical_differences_Q30, fig.width = 11, fig.height = 10, out.height='100%', out.width='100%', eval=any(grepl("dose", colnames(QAQC_annotated), ignore.case = T))}
print(ggdoseq30)
```

### Difference in Q30 scores between paired reads

```{r plot_chemical_differences_Q30_diff, fig.width = 11, fig.height = 10, out.height='100%', out.width='100%', eval=any(grepl("dose", colnames(QAQC_annotated), ignore.case = T))}
print(ggdoseq30diff)
```

### Percentage of reads mapping

```{r plot_chemical_differences_pct_mapping, fig.width = 11, fig.height = 10, out.height='100%', out.width='100%', eval=any(grepl("dose", colnames(QAQC_annotated), ignore.case = T))}
print(ggdosepctmapped)
```

### Total number of reads mapping

```{r plot_chemical_differences_NMR, fig.width = 11, fig.height = 10, out.height='100%', out.width='100%', eval=any(grepl("dose", colnames(QAQC_annotated), ignore.case = T))}
print(ggdosenmr)
```

### PC1 variance

```{r plot_chemical_differences_PC1, fig.width = 11, fig.height = 10, out.height='100%', out.width='100%', eval=any(grepl("dose", colnames(QAQC_annotated), ignore.case = T))}
print(ggdosepc1var)
```

### Ncov5

```{r plot_chemical_differences_Ncov5, fig.width = 11, fig.height = 10, out.height='100%', out.width='100%', eval=any(grepl("dose", colnames(QAQC_annotated), ignore.case = T))}
print(ggdosencov5)
```

### Nsig80

```{r plot_chemical_differences_Nsig80, fig.width = 11, fig.height = 10, out.height='100%', out.width='100%', eval=any(grepl("dose", colnames(QAQC_annotated), ignore.case = T))}
print(ggdosensig80)
```

### Gini coefficient

```{r plot_chemical_differences_Gini, fig.width = 11, fig.height = 10, out.height='100%', out.width='100%', eval=any(grepl("dose", colnames(QAQC_annotated), ignore.case = T))}
print(ggdosegini)
```

## {-}

## Outliers removed QC metrics {.tabset}

```{r plot_filter_criteria, fig.width = 11, fig.height = 10, out.height='100%', out.width='100%', eval=T}
ggq30 <-
  ggplot(dplyr::filter(QAQC_annotated, is.na(Any) |
                         Any != "FAIL"),
         aes(x = !!ensym(treatment_var), y = q30_data)) +
  geom_boxplot(outlier.shape = NA) +  # Remove outlier points from the boxplot itself
  geom_jitter(
    shape = 21,
    fill = "blue",
    position = position_jitter(width = 0.2, seed = 1)
  ) +  # Jittered points with consistent fill color
  geom_hline(yintercept = q30_cutoff * 100,
             linetype = "dotted",
             color = "red") +  # Add threshold line at q30_cutoff
  ylim((q30_cutoff * 100) - 1, 100) +  # Dynamic y-axis from 0 to max value
  ylab("Q30 Percentage") +  # Y-axis label
  xlab(as.character(treatment_var)) +  # X-axis label
  theme_minimal() +  # Minimal theme
  ggtitle("Q30 Data by Treatment")

ggq30diff <-
  ggplot(
    dplyr::filter(QAQC_annotated, is.na(Any) |
                    Any != "FAIL"),
    aes(x = !!ensym(treatment_var), y = diff_r1_r2_q30_data)
  ) +
  geom_boxplot(outlier.shape = NA) +  # Remove outlier points from the boxplot itself
  geom_jitter(
    shape = 21,
    fill = "blue",
    position = position_jitter(width = 0.2, seed = 1)
  ) +  # Jittered points with consistent fill color
  #geom_hline(yintercept = forward_reverse_q30_diff_cutoff*100, linetype = "dotted", color = "red") +  # Add threshold line at forward_reverse_q30_diff_cutoff
   ylim(
    0,
    if_else(
      condition = max(QAQC_annotated$diff_r1_r2_q30_data, na.rm = TRUE) > forward_reverse_q30_diff_cutoff * 100,
      true = max(QAQC_annotated$diff_r1_r2_q30_data, na.rm = TRUE) + 1,
      false = (forward_reverse_q30_diff_cutoff * 100) + 1
    )
  ) +  # Dynamic y-axis from 0 to max value asdfasdgasdgsaDGASD
  ylab("Difference in Q30 Between R1 and R2") +  # Y-axis label
  xlab(as.character(treatment_var)) +  # X-axis label
  theme_minimal() +  # Minimal theme
  ggtitle("Difference in Q30 Between R1 and R2 by Treatment")

ggpctmapped <-
  ggplot(
    dplyr::filter(QAQC_annotated, is.na(Any) |
                    Any != "FAIL"),
    aes(x = !!ensym(treatment_var), y = pct_mapped_data)
  ) +
  geom_boxplot(outlier.shape = NA) +  # Remove outlier points from the boxplot itself
  geom_jitter(
    shape = 21,
    fill = "blue",
    position = position_jitter(width = 0.2, seed = 1)
  ) +  # Jittered points with consistent fill color
  geom_hline(yintercept = align_threshold * 100,
             linetype = "dotted",
             color = "red") +  # Add threshold line at 70%
  ylim((align_threshold * 100) - 1, 100) +  # Dynamic y-axis from 0 to max value
  ylab("Percentage of Mapped Reads") +  # Y-axis label
  xlab(as.character(treatment_var)) +  # X-axis label
  theme_minimal() +  # Minimal theme
  ggtitle("Percentage of Mapped Reads by Treatment")

ggpc1var <-
  ggplot(
    dplyr::filter(QAQC_annotated, is.na(Any) |
                    Any != "FAIL"),
    aes(x = !!ensym(treatment_var), y = PC1_var_data)
  ) +
  geom_boxplot(outlier.shape = NA) +  # Remove outlier points from the boxplot itself
  geom_jitter(
    shape = 21,
    fill = "blue",
    position = position_jitter(width = 0.2, seed = 1)
  ) +  # Jittered points with consistent fill color
  ylim(0 - 1, max(QAQC_annotated$PC1_var_data, na.rm = TRUE) + 1) +  # Dynamic y-axis from 0 to max value
  ylab("PC1 Variance") +  # Y-axis label
  xlab(as.character(treatment_var)) +  # X-axis label
  theme_minimal() +  # Minimal theme
  ggtitle("Variance of PC1 Across Samples\nRelative to Median in Dose Groups by Treatment")

gggini <-
  ggplot(dplyr::filter(QAQC_annotated, is.na(Any) |
                         Any != "FAIL"),
         aes(x = !!ensym(treatment_var), y = Gini_data)) +
  geom_boxplot(outlier.shape = NA) +  # Remove outlier points from the boxplot itself
  geom_jitter(aes(fill = ifelse(Gini_data > gini_cutoff, "red", "black")),
              shape = 21,
              position = position_jitter(width = 0.2, seed = 1)) +  # Jittered points with conditional fill color
  geom_hline(yintercept = gini_cutoff,
             linetype = "dotted",
             color = "red") +  # Add threshold line at 0.95
  ylim(min(QAQC_annotated$Gini_data, na.rm = TRUE) - 0.01, 1) +  # Dynamic lower limit, upper limit set to 1
  ylab("Gini Coefficient") +  # Y-axis label
  xlab(as.character(treatment_var)) +  # X-axis label
  theme_minimal() +  # Minimal theme
  scale_fill_identity() +  # Maintain fill colors
  ggtitle("Gini Coefficient by Treatment")

ggnmr <-
  ggplot(dplyr::filter(QAQC_annotated, is.na(Any) |
                         Any != "FAIL"),
         aes(x = !!ensym(treatment_var), y = NMR_data)) +
  geom_boxplot(outlier.shape = NA) +  # Remove outlier points from the boxplot itself
  geom_jitter(
    shape = 21,
    fill = "blue",
    position = position_jitter(width = 0.2, seed = 1)
  ) +  # Jittered points with consistent fill color
  geom_hline(yintercept = nmr_threshold,
             linetype = "dotted",
             color = "red") +  # Add threshold line at nmr_threshold
  ylim(0, max(QAQC_annotated$NMR_data, na.rm = TRUE) + 1000) +  # Dynamic y-axis from 0 to max value
  ylab("Number of Mapped Reads") +  # Y-axis label
  xlab(as.character(treatment_var)) +  # X-axis label
  theme_minimal() +  # Minimal theme
  ggtitle("Number of Total Mapped Reads by Treatment")

ggncov5 <-
  ggplot(dplyr::filter(QAQC_annotated, is.na(Any) |
                         Any != "FAIL"),
         aes(x = !!ensym(treatment_var), y = Ncov5_data)) +
  geom_boxplot(outlier.shape = NA) +  # Remove outlier points from the boxplot itself
  geom_jitter(
    shape = 21,
    fill = "blue",
    position = position_jitter(width = 0.2, seed = 1)
  ) +  # Jittered points with consistent fill color
  ylim(
    min(QAQC_annotated$Ncov5_data, na.rm = TRUE) - 0.01,
    max(QAQC_annotated$Ncov5_data, na.rm = TRUE) + 0.01
  ) +  # Dynamic y-axis from 0 to max value
  ylab("Proportion of Active Probes (>5 Mapped Reads)") +  # Y-axis label
  xlab(as.character(treatment_var)) +  # X-axis label
  theme_minimal() +  # Minimal theme
  ggtitle("NCov5")

ggnsig80 <-
  ggplot(dplyr::filter(QAQC_annotated, is.na(Any) |
                         Any != "FAIL"),
         aes(x = !!ensym(treatment_var), y = Nsig80_data)) +
  geom_boxplot(outlier.shape = NA) +  # Remove outlier points from the boxplot itself
  geom_jitter(
    shape = 21,
    fill = "blue",
    position = position_jitter(width = 0.2, seed = 100)
  ) +  # Jittered points with consistent fill color
  ylim(
    min(QAQC_annotated$Nsig80_data, na.rm = TRUE) - 100,
    max(QAQC_annotated$Nsig80_data, na.rm = TRUE) + 1
  ) +  # Dynamic y-axis from 0 to max value
  ylab("# of Probes for 80% Signal") +  # Y-axis label
  xlab(as.character(treatment_var)) +  # X-axis label
  theme_minimal() +  # Minimal theme
  ggtitle("NSig80")

ggsave(gggini, file = paste0(paths$output, "/Gini_plot.pdf"))
ggsave(ggnmr, file = paste0(paths$output, "/Mapped_reads_plot.pdf"))
ggsave(ggncov5, file = paste0(paths$output, "/NCov5_plot.pdf"))
ggsave(ggnsig80, file = paste0(paths$output, "/NSig80_plot.pdf"))
ggsave(ggq30, file = paste0(paths$output, "/Q30_plot.pdf"))
ggsave(ggq30diff, file = paste0(paths$output, "/Q30_diff_plot.pdf"))
ggsave(ggpctmapped, file = paste0(paths$output, "/Pct_mapped_plot.pdf"))
ggsave(ggpc1var, file = paste0(paths$output, "/PC1_variance_plot.pdf"))
```

### Q30 Scores

```{r plot_filtered_Q30, fig.width = 11, fig.height = 10, out.height='100%', out.width='100%', eval=any(grepl("dose", colnames(QAQC_annotated), ignore.case = T))}
print(ggq30)
```

### Difference in Q30 scores between paired reads

```{r plot_filtered_Q30_diff, fig.width = 11, fig.height = 10, out.height='100%', out.width='100%', eval=any(grepl("dose", colnames(QAQC_annotated), ignore.case = T))}
print(ggq30diff)
```

### Percentage of reads mapping

```{r plot_filtered_pct_mapping, fig.width = 11, fig.height = 10, out.height='100%', out.width='100%', eval=any(grepl("dose", colnames(QAQC_annotated), ignore.case = T))}
print(ggpctmapped)
```

### Total number of reads mapping

```{r plot_filtered_NMR, fig.width = 11, fig.height = 10, out.height='100%', out.width='100%', eval=any(grepl("dose", colnames(QAQC_annotated), ignore.case = T))}
print(ggnmr)
```

### PC1 variance

```{r plot_filtered_PC1, fig.width = 11, fig.height = 10, out.height='100%', out.width='100%', eval=any(grepl("dose", colnames(QAQC_annotated), ignore.case = T))}
print(ggpc1var)
```

### Ncov5

```{r plot_filtered_Ncov5, fig.width = 11, fig.height = 10, out.height='100%', out.width='100%', eval=any(grepl("dose", colnames(QAQC_annotated), ignore.case = T))}
print(ggncov5)
```

### Nsig80

```{r plot_filtered_Nsig80, fig.width = 11, fig.height = 10, out.height='100%', out.width='100%', eval=any(grepl("dose", colnames(QAQC_annotated), ignore.case = T))}
print(ggnsig80)
```

### Gini coefficient

```{r plot_filtered_Gini, fig.width = 11, fig.height = 10, out.height='100%', out.width='100%', eval=any(grepl("dose", colnames(QAQC_annotated), ignore.case = T))}
print(gggini)
```

## {-}

# References {-}

If these methods were used in your study, please cite the following papers as appropriate:  

---
nocite: '@*'
---

<div id="refs"></div>

# Session Info

## Date the report was generated.

```{r reproducibility1, echo = FALSE}
## Date the report was generated
Sys.time()
```

## Parameters Used

From the list elements in each params${variable} used to generate this report.

```{r paramsList, echo = FALSE}
df <- as.data.frame(unlist(params))
names(df) <- "Parameter Value"
knitr::kable(as.data.frame(df), format = "markdown")
```

## Wallclock time spent generating the report.

```{r reproducibility2, echo = FALSE}
## Processing time in seconds
totalTime <- diff(c(startTime, Sys.time()))
round(totalTime, digits = 3)
```

## `R` session information.

```{r reproducibility3, echo = FALSE}
## Session info
options(width = 120)
session_info()
```

## Pandoc version used: `r rmarkdown::pandoc_version()`.

<div class="tocify-extend-page" data-unique="tocify-extend-page" style="height: 0;"></div>